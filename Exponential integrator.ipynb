{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b9743d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Import the package and set up the device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "from inspect import getfullargspec\n",
    "\n",
    "from typing import List, Tuple, Union, Callable, Dict, Iterable, Generator\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from torch import Tensor\n",
    "from collections import namedtuple\n",
    "from warnings import warn\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477d460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e524c5ed",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Some useful functions: \n",
    "\n",
    "def hairer_norm(tensor):\n",
    "    return tensor.abs().pow(2).mean().sqrt()\n",
    "\n",
    "def standardize_vf_call_signature(vector_field, order=1, defunc_wrap=False):\n",
    "    \"Ensures Callables or nn.Modules passed to `ODEProblems` and `NeuralODE` have consistent `__call__` signature (t, x)\"\n",
    "    \n",
    "    if issubclass(type(vector_field), nn.Module):\n",
    "        if 't' not in getfullargspec(vector_field.forward).args:\n",
    "            print(\"Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, \"\n",
    "                \"we've wrapped it for you.\")\n",
    "            vector_field = DEFuncBase(vector_field, has_time_arg=False)\n",
    "    else: \n",
    "        # argspec for lambda functions needs to be done on the function itself\n",
    "        if 't' not in getfullargspec(vector_field).args:\n",
    "            print(\"Your vector field callable (lambda) should have both time `t` and state `x` as arguments, \"\n",
    "                \"we've wrapped it for you.\")\n",
    "            vector_field = DEFuncBase(vector_field, has_time_arg=False)   \n",
    "        else: vector_field = DEFuncBase(vector_field, has_time_arg=True) \n",
    "    if defunc_wrap: return DEFunc(vector_field, order)\n",
    "    else: return vector_field\n",
    "    \n",
    "\n",
    "\n",
    "def init_step(f, f0, x0, t0, order, atol, rtol):\n",
    "    scale = atol + torch.abs(x0) * rtol\n",
    "    d0, d1 = hairer_norm(x0 / scale), hairer_norm(f0 / scale)\n",
    "\n",
    "    if d0 < 1e-5 or d1 < 1e-5:\n",
    "        h0 = torch.tensor(1e-6, dtype=t0.dtype, device=t0.device)\n",
    "    else:\n",
    "        h0 = 0.01 * d0 / d1\n",
    "\n",
    "    x_new = x0 + h0 * f0\n",
    "    f_new = f(t0 + h0, x_new)\n",
    "    d2 = hairer_norm((f_new - f0) / scale) / h0\n",
    "    if d1 <= 1e-15 and d2 <= 1e-15:\n",
    "        h1 = torch.max(torch.tensor(1e-6, dtype=t0.dtype, device=t0.device), h0 * 1e-3)\n",
    "    else:\n",
    "        h1 = (0.01 / max(d1, d2)) ** (1. / float(order + 1))\n",
    "    dt = torch.min(100 * h0, h1).to(t0)\n",
    "    return dt\n",
    "\n",
    "@torch.no_grad()\n",
    "def adapt_step(dt, error_ratio, safety, min_factor, max_factor, order):\n",
    "    if error_ratio == 0: return dt * max_factor\n",
    "    if error_ratio < 1: min_factor = torch.ones_like(dt)\n",
    "    exponent = torch.tensor(order, dtype=dt.dtype, device=dt.device).reciprocal()\n",
    "    factor = torch.min(max_factor, torch.max(safety / error_ratio ** exponent, min_factor))\n",
    "    return dt * factor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8846dd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Some useful class:\n",
    "\n",
    "class DEFuncBase(nn.Module):\n",
    "    def __init__(self, vector_field: Callable, has_time_arg: bool = True):\n",
    "        \"\"\"Basic wrapper to ensure call signature compatibility between generic torch Modules and vector fields.\n",
    "        Args:\n",
    "            vector_field (Callable): callable defining the dynamics / vector field / `dxdt` / forcing function\n",
    "            has_time_arg (bool, optional): Internal arg. to indicate whether the callable has `t` in its `__call__'\n",
    "                or `forward` method. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nfe, self.vf, self.has_time_arg = 0.0, vector_field, has_time_arg\n",
    "\n",
    "    def forward(self, t: Tensor, x: Tensor, args: Dict = {}) -> Tensor:\n",
    "        self.nfe += 1\n",
    "        if self.has_time_arg:\n",
    "            return self.vf(t, x, args=args)\n",
    "        else:\n",
    "            return self.vf(x)\n",
    "        \n",
    "\n",
    "class DEFunc(nn.Module):\n",
    "    def __init__(self, vector_field: Callable, order: int = 1):\n",
    "        \"\"\"Special vector field wrapper for Neural ODEs.\n",
    "\n",
    "        Handles auxiliary tasks: time (\"depth\") concatenation, higher-order dynamics and forward propagated integral losses.\n",
    "\n",
    "        Args:\n",
    "            vector_field (Callable): callable defining the dynamics / vector field / `dxdt` / forcing function\n",
    "            order (int, optional): order of the differential equation. Defaults to 1.\n",
    "\n",
    "        Notes:\n",
    "            Currently handles the following:\n",
    "            (1) assigns time tensor to each submodule requiring it (e.g. `GalLinear`).\n",
    "            (2) in case of integral losses + reverse-mode differentiation, propagates the loss in the first dimension of `x`\n",
    "                and automatically splits the Tensor into `x[:, 0]` and `x[:, 1:]` for vector field computation\n",
    "            (3) in case of higher-order dynamics, adjusts the vector field forward to recursively compute various orders.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vf, self.nfe, = vector_field, 0.0\n",
    "        self.order, self.integral_loss, self.sensitivity = order, None, None\n",
    "        # identify whether vector field already has time arg\n",
    "\n",
    "    def forward(self, t: Tensor, x: Tensor, args: Dict = {}) -> Tensor:\n",
    "        self.nfe += 1\n",
    "        # set `t` depth-variable to DepthCat modules\n",
    "        for _, module in self.vf.named_modules():\n",
    "            if hasattr(module, \"t\"):\n",
    "                module.t = t\n",
    "\n",
    "        # if-else to handle autograd training with integral loss propagated in x[:, 0]\n",
    "        if (self.integral_loss is not None) and self.sensitivity == \"autograd\":\n",
    "            x_dyn = x[:, 1:]\n",
    "            dlds = self.integral_loss(t, x_dyn)\n",
    "            if len(dlds.shape) == 1:\n",
    "                dlds = dlds[:, None]\n",
    "            if self.order > 1:\n",
    "                x_dyn = self.horder_forward(t, x_dyn, args)\n",
    "            else:\n",
    "                x_dyn = self.vf(t, x_dyn)\n",
    "            return cat([dlds, x_dyn], 1).to(x_dyn)\n",
    "\n",
    "        # regular forward\n",
    "        else:\n",
    "            if self.order > 1:\n",
    "                x = self.higher_order_forward(t, x)\n",
    "            else:\n",
    "                x = self.vf(t, x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d589cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a70a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fda41c",
   "metadata": {
    "code_folding": [
     0,
     21,
     23
    ]
   },
   "outputs": [],
   "source": [
    "## The table for DP45 and tsit5\n",
    "ExplicitRKTableau = namedtuple('ExplicitRKTableau', 'c, A, b_sol, b_err')\n",
    "\n",
    "\n",
    "def construct_dopri5(dtype):\n",
    "    c = torch.tensor([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1., 1.], dtype=dtype)\n",
    "    a = [\n",
    "        torch.tensor([1 / 5], dtype=dtype),\n",
    "        torch.tensor([3 / 40, 9 / 40], dtype=dtype),\n",
    "        torch.tensor([44 / 45, -56 / 15, 32 / 9], dtype=dtype),\n",
    "        torch.tensor([19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729], dtype=dtype),\n",
    "        torch.tensor([9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656], dtype=dtype),\n",
    "        torch.tensor([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84], dtype=dtype),\n",
    "    ]\n",
    "    bsol = torch.tensor([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0], dtype=dtype)\n",
    "    berr = torch.tensor([1951 / 21600, 0, 22642 / 50085, 451 / 720, -12231 / 42400, 649 / 6300, 1 / 60.], dtype=dtype)\n",
    "\n",
    "    dmid = torch.tensor([-1.1270175653862835, 0., 2.675424484351598, -5.685526961588504, 3.5219323679207912,\n",
    "                         -1.7672812570757455, 2.382468931778144])\n",
    "    return (c, a, bsol, bsol - berr)\n",
    "\n",
    "def construct_tsit5(dtype):\n",
    "\n",
    "    c = torch.tensor([\n",
    "        161 / 1000,\n",
    "        327 / 1000,\n",
    "        9 / 10,\n",
    "        .9800255409045096857298102862870245954942137979563024768854764293221195950761080302604,\n",
    "        1.,\n",
    "        1.\n",
    "    ], dtype=dtype)\n",
    "    a = [\n",
    "        torch.tensor([\n",
    "            161 / 1000\n",
    "        ], dtype=dtype),\n",
    "        torch.tensor([\n",
    "            -.8480655492356988544426874250230774675121177393430391537369234245294192976164141156943e-2,\n",
    "            .3354806554923569885444268742502307746751211773934303915373692342452941929761641411569\n",
    "        ], dtype=dtype),\n",
    "        torch.tensor([\n",
    "            2.897153057105493432130432594192938764924887287701866490314866693455023795137503079289,\n",
    "            -6.359448489975074843148159912383825625952700647415626703305928850207288721235210244366,\n",
    "            4.362295432869581411017727318190886861027813359713760212991062156752264926097707165077\n",
    "        ], dtype=dtype),\n",
    "        torch.tensor([\n",
    "            5.325864828439256604428877920840511317836476253097040101202360397727981648835607691791,\n",
    "            -11.74888356406282787774717033978577296188744178259862899288666928009020615663593781589,\n",
    "            7.495539342889836208304604784564358155658679161518186721010132816213648793440552049753,\n",
    "            -.9249506636175524925650207933207191611349983406029535244034750452930469056411389539635e-1\n",
    "        ], dtype=dtype),\n",
    "        torch.tensor([\n",
    "            5.861455442946420028659251486982647890394337666164814434818157239052507339770711679748,\n",
    "            -12.92096931784710929170611868178335939541780751955743459166312250439928519268343184452,\n",
    "            8.159367898576158643180400794539253485181918321135053305748355423955009222648673734986,\n",
    "            -.7158497328140099722453054252582973869127213147363544882721139659546372402303777878835e-1,\n",
    "            -.2826905039406838290900305721271224146717633626879770007617876201276764571291579142206e-1\n",
    "        ], dtype=dtype),\n",
    "        torch.tensor([\n",
    "            .9646076681806522951816731316512876333711995238157997181903319145764851595234062815396e-1,\n",
    "            1 / 100,\n",
    "            .4798896504144995747752495322905965199130404621990332488332634944254542060153074523509,\n",
    "            1.379008574103741893192274821856872770756462643091360525934940067397245698027561293331,\n",
    "            -3.290069515436080679901047585711363850115683290894936158531296799594813811049925401677,\n",
    "            2.324710524099773982415355918398765796109060233222962411944060046314465391054716027841\n",
    "        ], dtype=dtype),\n",
    "    ]\n",
    "    bsol = torch.tensor([\n",
    "        .9646076681806522951816731316512876333711995238157997181903319145764851595234062815396e-1,\n",
    "        1 / 100,\n",
    "        .4798896504144995747752495322905965199130404621990332488332634944254542060153074523509,\n",
    "        1.379008574103741893192274821856872770756462643091360525934940067397245698027561293331,\n",
    "        -3.290069515436080679901047585711363850115683290894936158531296799594813811049925401677,\n",
    "        2.324710524099773982415355918398765796109060233222962411944060046314465391054716027841,\n",
    "        0.\n",
    "    ], dtype=dtype)\n",
    "    berr = torch.tensor([\n",
    "        .9468075576583945807478876255758922856117527357724631226139574065785592789071067303271e-1,\n",
    "        .9183565540343253096776363936645313759813746240984095238905939532922955247253608687270e-2,\n",
    "        .4877705284247615707855642599631228241516691959761363774365216240304071651579571959813,\n",
    "        1.234297566930478985655109673884237654035539930748192848315425833500484878378061439761,\n",
    "        -2.707712349983525454881109975059321670689605166938197378763992255714444407154902012702,\n",
    "        1.866628418170587035753719399566211498666255505244122593996591602841258328965767580089,\n",
    "        1 / 66.,\n",
    "    ], dtype=dtype)\n",
    "    return (c, a, bsol, bsol - berr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76065f88",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Differential solver class\n",
    "class DiffEqSolver(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            order, \n",
    "            stepping_class:str=\"fixed\", \n",
    "            min_factor:float=0.2, \n",
    "            max_factor:float=10, \n",
    "            safety:float=0.9\n",
    "        ):\n",
    "\n",
    "        super(DiffEqSolver, self).__init__()\n",
    "        self.order = order\n",
    "        self.min_factor = torch.tensor([min_factor])\n",
    "        self.max_factor = torch.tensor([max_factor])\n",
    "        self.safety = torch.tensor([safety])\n",
    "        self.tableau = None\n",
    "        self.stepping_class = stepping_class\n",
    "\n",
    "    def sync_device_dtype(self, x, t_span):\n",
    "        \"Ensures `x`, `t_span`, `tableau` and other solver tensors are on the same device with compatible dtypes\"\n",
    "        device = x.device\n",
    "        if self.tableau is not None:\n",
    "            c, a, bsol, berr = self.tableau\n",
    "            self.tableau = c.to(x), [a.to(x) for a in a], bsol.to(x), berr.to(x)\n",
    "        t_span = t_span.to(device)\n",
    "        self.safety = self.safety.to(device)\n",
    "        self.min_factor = self.min_factor.to(device)\n",
    "        self.max_factor = self.max_factor.to(device)\n",
    "        return x, t_span\n",
    "\n",
    "    def step(self, f, x, t, dt, k1=None, args=None):\n",
    "        raise NotImplementedError(\"Stepping rule not implemented for the solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d595e62",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Copy the hardcoded DP45 method\n",
    "class DormandPrince45(DiffEqSolver):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__(order=5)\n",
    "        self.dtype = dtype\n",
    "        self.stepping_class = 'fixed'\n",
    "        self.tableau = construct_dopri5(self.dtype)\n",
    "\n",
    "    def step(self, f, x, t, dt, k1=None, args=None):\n",
    "        c, a, bsol, berr = self.tableau\n",
    "        if k1 == None: k1 = f(t, x)\n",
    "        k2 = f(t + c[0] * dt, x + dt * a[0] * k1)\n",
    "        k3 = f(t + c[1] * dt, x + dt * (a[1][0] * k1 + a[1][1] * k2))\n",
    "        k4 = f(t + c[2] * dt, x + dt * a[2][0] * k1 + dt * a[2][1] * k2 + dt * a[2][2] * k3)\n",
    "        k5 = f(t + c[3] * dt, x + dt * a[3][0] * k1 + dt * a[3][1] * k2 + dt * a[3][2] * k3 + dt * a[3][3] * k4)\n",
    "        k6 = f(t + c[4] * dt, x + dt * a[4][0] * k1 + dt * a[4][1] * k2 + dt * a[4][2] * k3 + dt * a[4][3] * k4 + dt * a[4][4] * k5)\n",
    "        k7 = f(t + c[5] * dt, x + dt * a[5][0] * k1 + dt * a[5][1] * k2 + dt * a[5][2] * k3 + dt * a[5][3] * k4 + dt * a[5][4] * k5 + dt * a[5][5] * k6)\n",
    "        x_sol = x + dt * (bsol[0] * k1 + bsol[1] * k2 + bsol[2] * k3 + bsol[3] * k4 + bsol[4] * k5 + bsol[5] * k6)\n",
    "        err = dt * (berr[0] * k1 + berr[1] * k2 + berr[2] * k3 + berr[3] * k4 + berr[4] * k5 + berr[5] * k6 + berr[6] * k7)\n",
    "        return k7, x_sol, err, (k1, k2, k3, k4, k5, k6, k7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18491812",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Copy the hardcoded Tsitouras45 method\n",
    "class Tsitouras45(DiffEqSolver):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__(order=5)\n",
    "        self.dtype = dtype\n",
    "        self.stepping_class = 'adaptive'\n",
    "        self.tableau = construct_tsit5(self.dtype)\n",
    "\n",
    "    def step(self, f, x, t, dt, k1=None, args=None) -> Tuple:\n",
    "        c, a, bsol, berr = self.tableau\n",
    "        if k1 == None: k1 = f(t, x)\n",
    "        k2 = f(t + c[0] * dt, x + dt * a[0][0] * k1)\n",
    "        k3 = f(t + c[1] * dt, x + dt * (a[1][0] * k1 + a[1][1] * k2))\n",
    "        k4 = f(t + c[2] * dt, x + dt * a[2][0] * k1 + dt * a[2][1] * k2 + dt * a[2][2] * k3)\n",
    "        k5 = f(t + c[3] * dt, x + dt * a[3][0] * k1 + dt * a[3][1] * k2 + dt * a[3][2] * k3 + dt * a[3][3] * k4)\n",
    "        k6 = f(t + c[4] * dt, x + dt * a[4][0] * k1 + dt * a[4][1] * k2 + dt * a[4][2] * k3 + dt * a[4][3] * k4 + dt * a[4][4] * k5)\n",
    "        k7 = f(t + c[5] * dt, x + dt * a[5][0] * k1 + dt * a[5][1] * k2 + dt * a[5][2] * k3 + dt * a[5][3] * k4 + dt * a[5][4] * k5 + dt * a[5][5] * k6)\n",
    "        x_sol = x + dt * (bsol[0] * k1 + bsol[1] * k2 + bsol[2] * k3 + bsol[3] * k4 + bsol[4] * k5 + bsol[5] * k6)\n",
    "        err = dt * (berr[0] * k1 + berr[1] * k2 + berr[2] * k3 + berr[3] * k4 + berr[4] * k5 + berr[5] * k6 + berr[6] * k7)\n",
    "        return k7, x_sol, err, (k1, k2, k3, k4, k5, k6, k7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052f348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01badd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb568c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b60c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421578d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d97aed",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Define the fixed time solver:\n",
    "def _fixed_odeint(f, x, t_span, solver, save_at=(), args={}):\n",
    "    \"\"\"Solves IVPs with same `t_span`, using fixed-step methods\"\"\"\n",
    "    if len(save_at) == 0: save_at = t_span\n",
    "    if not isinstance(save_at, torch.Tensor):\n",
    "        save_at = torch.tensor(save_at)\n",
    "\n",
    "    assert all(torch.isclose(t, save_at).sum() == 1 for t in save_at),\\\n",
    "        \"each element of save_at [torch.Tensor] must be contained in t_span [torch.Tensor] once and only once\"\n",
    "\n",
    "    t, T, dt = t_span[0], t_span[-1], t_span[1] - t_span[0]\n",
    "\n",
    "    sol = []\n",
    "    if torch.isclose(t, save_at).sum():\n",
    "        sol = [x]\n",
    "\n",
    "    steps = 1\n",
    "    while steps <= len(t_span) - 1:\n",
    "        _, x, _,_ = solver.step(f, x.squeeze(0), t.squeeze(0), dt, k1=None, args=args)\n",
    "        t = t + dt\n",
    "\n",
    "        if torch.isclose(t, save_at).sum():\n",
    "            sol.append(x)\n",
    "        if steps < len(t_span) - 1: dt = t_span[steps+1] - t\n",
    "        steps += 1\n",
    "\n",
    "    if isinstance(sol[0], dict):\n",
    "        final_out = {k: [v] for k, v in sol[0].items()}\n",
    "        _ = [final_out[k].append(x[k]) for k in x.keys() for x in sol[1:]]\n",
    "        final_out = {k: torch.stack(v) for k, v in final_out.items()}\n",
    "    elif isinstance(sol[0], torch.Tensor):\n",
    "        final_out = torch.stack(sol)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{type(x)} is not supported as the state variable\")\n",
    "\n",
    "    return save_at, final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dabceec",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Introduce the adaptive method\n",
    "def _adaptive_odeint(f, k1, x, dt, t_span, solver, atol=1e-4, rtol=1e-4, args=None, interpolator=None, return_all_eval=False, seminorm=(False, None)):\n",
    "    \"\"\"Adaptive ODE solve routine, called by `odeint`.\n",
    "\n",
    "    Args:\n",
    "        f ([type]):\n",
    "        k1 ([type]):\n",
    "        x ([type]):\n",
    "        dt ([type]):\n",
    "        t_span ([type]):\n",
    "        solver ([type]):\n",
    "        atol ([type], optional): Defaults to 1e-4.\n",
    "        rtol ([type], optional): Defaults to 1e-4.\n",
    "        args (Dict):\n",
    "        use_interp (bool, optional):\n",
    "        return_all_eval (bool, optional): Defaults to False.\n",
    "\n",
    "\n",
    "    Notes:\n",
    "        (1) We check if the user wants all evaluated solution points, not only those\n",
    "        corresponding to times in `t_span`. This is automatically set to `True` when `odeint`\n",
    "        is called for interpolated adjoints\n",
    "    \"\"\"\n",
    "    x, t_span = solver.sync_device_dtype(x, t_span)\n",
    "    t_eval, t, T = t_span[1:], t_span[:1], t_span[-1]\n",
    "    ckpt_counter, ckpt_flag = 0, False\n",
    "    eval_times, sol = [t], [x]\n",
    "    while t < T:\n",
    "        if t + dt > T:\n",
    "            dt = T - t\n",
    "        ############### checkpointing ###############################\n",
    "        if t_eval is not None:\n",
    "            # satisfy checkpointing by using interpolation scheme or resetting `dt`\n",
    "            if (ckpt_counter < len(t_eval)) and (t + dt > t_eval[ckpt_counter]):\n",
    "                if interpolator == None:\n",
    "                    # save old dt, raise \"checkpoint\" flag and repeat step\n",
    "                    dt_old, ckpt_flag = dt, True\n",
    "                    dt = t_eval[ckpt_counter] - t\n",
    "\n",
    "        f_new, x_new, x_err, stages = solver.step(f, x, t, dt, k1=k1, args=args)\n",
    "        ################# compute error #############################\n",
    "        if seminorm[0] == True:\n",
    "            state_dim = seminorm[1]\n",
    "            error = x_err[:state_dim]\n",
    "            error_scaled = error / (atol + rtol * torch.max(x[:state_dim].abs(), x_new[:state_dim].abs()))\n",
    "        else:\n",
    "            error = x_err\n",
    "            error_scaled = error / (atol + rtol * torch.max(x.abs(), x_new.abs()))\n",
    "        error_ratio = hairer_norm(error_scaled)\n",
    "        accept_step = error_ratio <= 1\n",
    "\n",
    "        if accept_step:\n",
    "            ############### checkpointing via interpolation ###############################\n",
    "            if t_eval is not None and interpolator is not None:\n",
    "                coefs = None\n",
    "                while (ckpt_counter < len(t_eval)) and (t + dt > t_eval[ckpt_counter]):\n",
    "                    t0, t1 = t, t + dt\n",
    "                    x_mid = x + dt * sum([interpolator.bmid[i] * stages[i] for i in range(len(stages))])\n",
    "                    f0, f1, x0, x1 = k1, f_new, x, x_new\n",
    "                    if coefs == None: coefs = interpolator.fit(dt, f0, f1, x0, x1, x_mid)\n",
    "                    x_in = interpolator.evaluate(coefs, t0, t1, t_eval[ckpt_counter])\n",
    "                    sol.append(x_in)\n",
    "                    eval_times.append(t_eval[ckpt_counter][None])\n",
    "                    ckpt_counter += 1\n",
    "\n",
    "            if t + dt == t_eval[ckpt_counter] or return_all_eval: # note (1)\n",
    "                sol.append(x_new)\n",
    "                eval_times.append(t + dt)\n",
    "                # we only increment the ckpt counter if the solution points corresponds to a time point in `t_span`\n",
    "                if t + dt == t_eval[ckpt_counter]: ckpt_counter += 1\n",
    "            t, x = t + dt, x_new\n",
    "            k1 = f_new\n",
    "\n",
    "        ################ stepsize control ###########################\n",
    "        # reset \"dt\" in case of checkpoint without interp\n",
    "        if ckpt_flag:\n",
    "            dt = dt_old - dt\n",
    "            ckpt_flag = False\n",
    "        dt = adapt_step(dt, error_ratio,\n",
    "                        solver.safety,\n",
    "                        solver.min_factor,\n",
    "                        solver.max_factor,\n",
    "                        solver.order)\n",
    "    return torch.cat(eval_times), torch.stack(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc705b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a867e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd12ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "854de9c1",
   "metadata": {},
   "source": [
    "## Test fixed step EPI2 with one step DP45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fd19aa",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Next, Implement the EPI2 method by only one step forward using DP45 as a sub-class of DP45:\n",
    "class EPI2_one_step(DormandPrince45):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "        self.stepping_class = 'fixed'\n",
    "\n",
    "    def step(self, f, x_n, t, dt, k1=None, args=None):\n",
    "        \n",
    "        init_cond_epi=torch.zeros_like(x_n)\n",
    "        constants=f(t,x_n).detach()\n",
    "        \n",
    "        ## Recall that this step of EPI2 is going to update x to x+dt using EPI2 step under dynamics f\n",
    "        def f_combined(t, x):\n",
    "            x.requires_grad_(True)\n",
    "            # Compute the original dynamics f(x, t)\n",
    "            f_x_t = f(t, x).detach()\n",
    "\n",
    "            # Compute the Jacobian-vector product (A_n * x) using torch.autograd.functional.jacobian\n",
    "            _, A_x_product = torch.autograd.functional.jvp(lambda x: f(t, x), x, x) \n",
    "            return A_x_product + constants\n",
    "        \n",
    "        _, increment, _, _=super().step(f_combined,init_cond_epi,t,dt)\n",
    "        x_sol=x_n+increment\n",
    "        return None, x_sol, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220eea22",
   "metadata": {},
   "source": [
    "## Test fixed step EPI2 with adaptive step DP45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44e50f7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Next, Implement the EPI2 method by applying a adaptive method using DP45 as a sub-class of DP45:\n",
    "class EPI2_adaptive(DormandPrince45):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__(dtype=dtype)\n",
    "        self.dtype = dtype\n",
    "        self.stepping_class = 'fixed'\n",
    "\n",
    "\n",
    "    def step(self, f, x_n, t, dt, k1=None, args=None):\n",
    "        \n",
    "        ## Specify what is solver we will be using\n",
    "        solver=DormandPrince45(dtype=torch.float32)\n",
    "        constants=f(t,x_n).detach()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Recall that this step of EPI2 is going to update x to x+dt using EPI2 step under dynamics f\n",
    "        def f_combined(t, x):\n",
    "            # Compute the original dynamics f(x, t)\n",
    "            x.requires_grad_(True)\n",
    "            f_x_t = f(t, x)\n",
    "\n",
    "            # Compute the Jacobian-vector product (A_n * x) using torch.autograd.functional.jacobian\n",
    "            _, A_x_product = torch.autograd.functional.jvp(lambda x: f(t, x), x, x)\n",
    "            return A_x_product + constants\n",
    "        \n",
    "        x0=torch.zeros_like(x_n).to(device)\n",
    "        t0=torch.zeros_like(t).to(device)\n",
    "        k0=f_combined(t0,x0).to(device)\n",
    "        dt_substep=init_step(f_combined, k0, x0, t0, solver.order, atol=1e-4, rtol=1e-4).to(device)\n",
    "        \n",
    "        t_span=torch.linspace(t0.item(), dt.item(), steps=5, dtype=torch.float32).to(device)\n",
    "\n",
    "        record_time, increment=_adaptive_odeint(f_combined, k0, x0, dt_substep, t_span, solver , atol=1e-4, rtol=1e-4)\n",
    "        \n",
    "        x_sol=x_n+increment[-1]\n",
    "        return None, x_sol, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd555bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e59ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b1d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b7766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433b5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11cbbb4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## introduce the odeint function:\n",
    "def odeint(f:Callable, x:Tensor, t_span:Union[List, Tensor], solver:Union[str, nn.Module], atol:float=1e-3, rtol:float=1e-3,\n",
    "           t_stops:Union[List, Tensor, None]=None, verbose:bool=False, interpolator:Union[str, Callable, None]=None, return_all_eval:bool=False,\n",
    "           save_at:Union[Iterable, Tensor]=(), args:Dict={}, seminorm:Tuple[bool, Union[int, None]]=(False, None)) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Solve an initial value problem (IVP) determined by function `f` and initial condition `x`.\n",
    "\n",
    "       Functional `odeint` API of the `torchdyn` package.\n",
    "\n",
    "    Args:\n",
    "        f (Callable):\n",
    "        x (Tensor):\n",
    "        t_span (Union[List, Tensor]):\n",
    "        solver (Union[str, nn.Module]):\n",
    "        atol (float, optional): Defaults to 1e-3.\n",
    "        rtol (float, optional): Defaults to 1e-3.\n",
    "        t_stops (Union[List, Tensor, None], optional): Defaults to None.\n",
    "        verbose (bool, optional): Defaults to False.\n",
    "        interpolator (bool, optional): Defaults to False.\n",
    "        return_all_eval (bool, optional): Defaults to False.\n",
    "        save_at (Union[List, Tensor], optional): Defaults to t_span\n",
    "        args (Dict): Arbitrary parameters used in step\n",
    "        seminorm (Tuple[bool, Union[int, None]], optional): Whether to use seminorms in local error computation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: returns a Tuple (t_eval, solution).\n",
    "    \"\"\"\n",
    "    if t_span[1] < t_span[0]: # time is reversed\n",
    "        if verbose: warn(\"You are integrating on a reversed time domain, adjusting the vector field automatically\")\n",
    "        f_ = lambda t, x: -f(-t, x)\n",
    "        t_span = -t_span\n",
    "    else: f_ = f\n",
    "\n",
    "    if type(t_span) == list: t_span = torch.cat(t_span)\n",
    "    # instantiate the solver in case the user has specified preference via a `str` and ensure compatibility of device ~ dtype\n",
    "    if type(solver) == str:\n",
    "        solver = str_to_solver(solver, x.dtype)\n",
    "    x, t_span = solver.sync_device_dtype(x, t_span)\n",
    "    stepping_class = solver.stepping_class\n",
    "\n",
    "    # instantiate the interpolator similar to the solver steps above\n",
    "    if isinstance(solver, Tsitouras45):\n",
    "        if verbose: warn(\"Running interpolation not yet implemented for `tsit5`\")\n",
    "        interpolator = None\n",
    "\n",
    "    if type(interpolator) == str:\n",
    "        interpolator = str_to_interp(interpolator, x.dtype)\n",
    "        x, t_span = interpolator.sync_device_dtype(x, t_span)\n",
    "\n",
    "    # access parallel integration routines with different t_spans for each sample in `x`.\n",
    "    if len(t_span.shape) > 1:\n",
    "        raise NotImplementedError(\"Parallel routines not implemented yet, check experimental versions of `torchdyn`\")\n",
    "    # odeint routine with a single t_span for all samples\n",
    "    elif len(t_span.shape) == 1:\n",
    "        if stepping_class == 'fixed':\n",
    "            if atol != odeint.__defaults__[0] or rtol != odeint.__defaults__[1]:\n",
    "                warn(\"Setting tolerances has no effect on fixed-step methods\")\n",
    "            # instantiate save_at tensor\n",
    "            return _fixed_odeint(f_, x, t_span, solver, save_at=save_at, args=args)\n",
    "        elif stepping_class == 'adaptive':\n",
    "            t = t_span[0]\n",
    "            k1 = f_(t, x)\n",
    "            dt = init_step(f, k1, x, t, solver.order, atol, rtol)\n",
    "            if len(save_at) > 0: warn(\"Setting save_at has no effect on adaptive-step methods\")\n",
    "            return _adaptive_odeint(f_, k1, x, dt, t_span, solver, atol, rtol, args, interpolator, return_all_eval, seminorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33b14a54",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Define the customized forward and backward method under autograd.function class:\n",
    "def generic_odeint(problem_type, vf, x, t_span, solver, atol, rtol, interpolator, B0=None, \n",
    "                  return_all_eval=False, maxiter=4, fine_steps=4, save_at=()):\n",
    "    \"Dispatches to appropriate `odeint` function depending on `Problem` class (ODEProblem, MultipleShootingProblem)\"\n",
    "    if problem_type == 'standard':\n",
    "        return odeint(vf, x, t_span, solver, atol=atol, rtol=rtol, interpolator=interpolator, return_all_eval=return_all_eval,\n",
    "                      save_at=save_at)\n",
    "    elif problem_type == 'multiple_shooting':\n",
    "        return odeint_mshooting(vf, x, t_span, solver, B0=B0, fine_steps=fine_steps, maxiter=maxiter)\n",
    "\n",
    "\n",
    "# TODO: optimize and make conditional gradient computations w.r.t end times\n",
    "# TODO: link `seminorm` arg from `ODEProblem`\n",
    "def _gather_odefunc_adjoint(vf, vf_params, solver, atol, rtol, interpolator, solver_adjoint, \n",
    "                            atol_adjoint, rtol_adjoint, integral_loss, problem_type, maxiter=4, fine_steps=4):\n",
    "    \"Prepares definition of autograd.Function for adjoint sensitivity analysis of the above `ODEProblem`\"\n",
    "    class _ODEProblemFunc(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, vf_params, x, t_span, B=None, save_at=()):\n",
    "            t_sol, sol = generic_odeint(problem_type, vf, x, t_span, solver, atol, rtol, interpolator, B, \n",
    "                                        False, maxiter, fine_steps, save_at)\n",
    "            ctx.save_for_backward(sol, t_sol)\n",
    "            return t_sol, sol\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, *grad_output):\n",
    "            sol, t_sol = ctx.saved_tensors\n",
    "            vf_params = torch.cat([p.contiguous().flatten() for p in vf.parameters()])\n",
    "            # initialize flattened adjoint state\n",
    "            xT, λT, μT = sol[-1], grad_output[-1][-1], torch.zeros_like(vf_params)\n",
    "            xT_nel, λT_nel, μT_nel = xT.numel(), λT.numel(), μT.numel()\n",
    "            xT_shape, λT_shape, μT_shape = xT.shape, λT.shape, μT.shape\n",
    "\n",
    "            λT_flat = λT.flatten()\n",
    "            λtT = λT_flat @ vf(t_sol[-1], xT).flatten()\n",
    "            # concatenate all states of adjoint system\n",
    "            A = torch.cat([xT.flatten(), λT_flat, μT.flatten(), λtT[None]])\n",
    "\n",
    "            def adjoint_dynamics(t, A):\n",
    "                if len(t.shape) > 0: t = t[0]\n",
    "                x, λ, μ = A[:xT_nel], A[xT_nel:xT_nel+λT_nel], A[-μT_nel-1:-1]\n",
    "                x, λ, μ = x.reshape(xT.shape), λ.reshape(λT.shape), μ.reshape(μT.shape)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    x, t = x.requires_grad_(True), t.requires_grad_(True)\n",
    "                    dx = vf(t, x)\n",
    "                    dλ, dt, *dμ = tuple(torch.autograd.grad(dx, (x, t) + tuple(vf.parameters()), -λ,\n",
    "                                    allow_unused=True, retain_graph=True))\n",
    "\n",
    "                    if integral_loss:\n",
    "                        dg = torch.autograd.grad(integral_loss(t, x).sum(), x, allow_unused=True, retain_graph=True)[0]\n",
    "                        dλ = dλ - dg\n",
    "\n",
    "                    dμ = torch.cat([el.flatten() if el is not None else torch.zeros(1) \n",
    "                                    for el in dμ], dim=-1)\n",
    "                    if dt == None: dt = torch.zeros(1).to(t)\n",
    "                    if len(t.shape) == 0: dt = dt.unsqueeze(0)\n",
    "                return torch.cat([dx.flatten(), dλ.flatten(), dμ.flatten(), dt.flatten()])\n",
    "\n",
    "            # solve the adjoint equation\n",
    "            n_elements = (xT_nel, λT_nel, μT_nel)\n",
    "            dLdt = torch.zeros(len(t_sol)).to(xT)\n",
    "            dLdt[-1] = λtT\n",
    "            for i in range(len(t_sol) - 1, 0, -1):\n",
    "                t_adj_sol, A = odeint(adjoint_dynamics, A, t_sol[i - 1:i + 1].flip(0), \n",
    "                                      solver_adjoint, atol=atol_adjoint, rtol=rtol_adjoint,\n",
    "                                      seminorm=(True, xT_nel+λT_nel))\n",
    "                # prepare adjoint state for next interval\n",
    "                #TODO: reuse vf_eval for dLdt calculations\n",
    "                xt = A[-1, :xT_nel].reshape(xT_shape)\n",
    "                dLdt_ = A[-1, xT_nel:xT_nel + λT_nel]@vf(t_sol[i], xt).flatten()\n",
    "                A[-1, -1:] -= grad_output[0][i - 1]\n",
    "                dLdt[i-1] = dLdt_\n",
    "\n",
    "                A = torch.cat([A[-1, :xT_nel], A[-1, xT_nel:xT_nel + λT_nel], A[-1, -μT_nel-1:-1], A[-1, -1:]])\n",
    "                A[xT_nel:xT_nel + λT_nel] += grad_output[-1][i - 1].flatten()\n",
    "\n",
    "            λ, μ = A[xT_nel:xT_nel + λT_nel], A[-μT_nel-1:-1]\n",
    "            λ, μ = λ.reshape(λT.shape), μ.reshape(μT.shape)\n",
    "            λ_tspan = torch.stack([dLdt[0], dLdt[-1]])\n",
    "            return (μ, λ, λ_tspan, None, None, None)\n",
    "\n",
    "    return _ODEProblemFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e04b5a",
   "metadata": {
    "code_folding": [
     0,
     121
    ]
   },
   "outputs": [],
   "source": [
    "## Define the ODEproblem class for bridge\n",
    "class ODEProblem(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_field: Union[Callable, nn.Module],\n",
    "        solver: Union[str, nn.Module],\n",
    "        interpolator: Union[str, Callable, None] = None,\n",
    "        order: int = 1,\n",
    "        atol: float = 1e-4,\n",
    "        rtol: float = 1e-4,\n",
    "        sensitivity: str = \"autograd\",\n",
    "        solver_adjoint: Union[str, nn.Module, None] = None,\n",
    "        atol_adjoint: float = 1e-6,\n",
    "        rtol_adjoint: float = 1e-6,\n",
    "        seminorm: bool = False,\n",
    "        integral_loss: Union[Callable, None] = None,\n",
    "        optimizable_params: Union[Iterable, Generator] = (),\n",
    "    ):\n",
    "        \"\"\"An ODE Problem coupling a given vector field with solver and sensitivity algorithm to compute gradients w.r.t different quantities.\n",
    "\n",
    "        Args:\n",
    "            vector_field ([Callable]): the vector field, called with `vector_field(t, x)` for `vector_field(x)`.\n",
    "                                       In the second case, the Callable is automatically wrapped for consistency\n",
    "            solver (Union[str, nn.Module]):\n",
    "            order (int, optional): Order of the ODE. Defaults to 1.\n",
    "            atol (float, optional): Absolute tolerance of the solver. Defaults to 1e-4.\n",
    "            rtol (float, optional): Relative tolerance of the solver. Defaults to 1e-4.\n",
    "            sensitivity (str, optional): Sensitivity method ['autograd', 'adjoint', 'interpolated_adjoint']. Defaults to 'autograd'.\n",
    "            solver_adjoint (Union[str, nn.Module, None], optional): ODE solver for the adjoint. Defaults to None.\n",
    "            atol_adjoint (float, optional): Defaults to 1e-6.\n",
    "            rtol_adjoint (float, optional): Defaults to 1e-6.\n",
    "            seminorm (bool, optional): Indicates whether the a seminorm should be used for error estimation during adjoint backsolves. Defaults to False.\n",
    "            integral_loss (Union[Callable, None]): Integral loss to optimize for. Defaults to None.\n",
    "            optimizable_parameters (Union[Iterable, Generator]): parameters to calculate sensitivies for. Defaults to ().\n",
    "        Notes:\n",
    "            Integral losses can be passed as generic function or `nn.Modules`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # instantiate solver at initialization\n",
    "        if type(solver) == str:\n",
    "            solver = str_to_solver(solver)\n",
    "        if solver_adjoint is None:\n",
    "            solver_adjoint = solver\n",
    "        else:\n",
    "            solver_adjoint = str_to_solver(solver_adjoint)\n",
    "\n",
    "        self.solver, self.interpolator, self.atol, self.rtol = (\n",
    "            solver,\n",
    "            interpolator,\n",
    "            atol,\n",
    "            rtol,\n",
    "        )\n",
    "        self.solver_adjoint, self.atol_adjoint, self.rtol_adjoint = (\n",
    "            solver_adjoint,\n",
    "            atol_adjoint,\n",
    "            rtol_adjoint,\n",
    "        )\n",
    "        self.sensitivity, self.integral_loss = sensitivity, integral_loss\n",
    "\n",
    "        # wrap vector field if `t, x` is not the call signature\n",
    "        vector_field = standardize_vf_call_signature(vector_field)\n",
    "\n",
    "        self.vf, self.order, self.sensalg = vector_field, order, sensitivity\n",
    "        optimizable_params = tuple(optimizable_params)\n",
    "\n",
    "        if len(tuple(self.vf.parameters())) > 0:\n",
    "            self.vf_params = torch.cat(\n",
    "                [p.contiguous().flatten() for p in self.vf.parameters()]\n",
    "            )\n",
    "\n",
    "        elif len(optimizable_params) > 0:\n",
    "            # use `optimizable_parameters` if f itself does not have a .parameters() iterable\n",
    "            # TODO: advanced logic to retain naming in case `state_dicts()` are passed\n",
    "            for k, p in enumerate(optimizable_params):\n",
    "                self.vf.register_parameter(f\"optimizable_parameter_{k}\", p)\n",
    "            self.vf_params = torch.cat(\n",
    "                [p.contiguous().flatten() for p in optimizable_params]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"Your vector field does not have `nn.Parameters` to optimize.\")\n",
    "            dummy_parameter = nn.Parameter(torch.zeros(1))\n",
    "            self.vf.register_parameter(\"dummy_parameter\", dummy_parameter)\n",
    "            self.vf_params = torch.cat(\n",
    "                [p.contiguous().flatten() for p in self.vf.parameters()]\n",
    "            )\n",
    "\n",
    "    def _autograd_func(self):\n",
    "        \"create autograd functions for backward pass\"\n",
    "        self.vf_params = torch.cat(\n",
    "            [p.contiguous().flatten() for p in self.vf.parameters()]\n",
    "        )\n",
    "        if (self.sensalg == \"adjoint\"):  # alias .apply as direct call to preserve consistency of call signature\n",
    "            return _gather_odefunc_adjoint(\n",
    "                self.vf,\n",
    "                self.vf_params,\n",
    "                self.solver,\n",
    "                self.atol,\n",
    "                self.rtol,\n",
    "                self.interpolator,\n",
    "                self.solver_adjoint,\n",
    "                self.atol_adjoint,\n",
    "                self.rtol_adjoint,\n",
    "                self.integral_loss,\n",
    "                problem_type=\"standard\",\n",
    "            ).apply\n",
    "        elif self.sensalg == \"interpolated_adjoint\":\n",
    "            return _gather_odefunc_interp_adjoint(\n",
    "                self.vf,\n",
    "                self.vf_params,\n",
    "                self.solver,\n",
    "                self.atol,\n",
    "                self.rtol,\n",
    "                self.interpolator,\n",
    "                self.solver_adjoint,\n",
    "                self.atol_adjoint,\n",
    "                self.rtol_adjoint,\n",
    "                self.integral_loss,\n",
    "                problem_type=\"standard\",\n",
    "            ).apply\n",
    "\n",
    "    def odeint(self, x: Tensor, t_span: Tensor, save_at: Tensor = (), args={}):\n",
    "        \"Returns Tuple(`t_eval`, `solution`)\"\n",
    "        if self.sensalg == \"autograd\":\n",
    "            return odeint(\n",
    "                self.vf,\n",
    "                x,\n",
    "                t_span,\n",
    "                self.solver,\n",
    "                self.atol,\n",
    "                self.rtol,\n",
    "                interpolator=self.interpolator,\n",
    "                save_at=save_at,\n",
    "                args=args,\n",
    "            )\n",
    "        else:\n",
    "            return self._autograd_func()(self.vf_params, x, t_span, save_at, args)\n",
    "\n",
    "    def forward(self, x: Tensor, t_span: Tensor, save_at: Tensor = (), args={}):\n",
    "        \"For safety redirects to intended method `odeint`\"\n",
    "        return self.odeint(x, t_span, save_at, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23015be",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Define the NeuralODE class\n",
    "class NeuralODE(ODEProblem, pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_field: Union[Callable, nn.Module],\n",
    "        solver: Union[str, nn.Module] = \"tsit5\",\n",
    "        order: int = 1,\n",
    "        atol: float = 1e-3,\n",
    "        rtol: float = 1e-3,\n",
    "        sensitivity=\"autograd\",\n",
    "        solver_adjoint: Union[str, nn.Module, None] = None,\n",
    "        atol_adjoint: float = 1e-4,\n",
    "        rtol_adjoint: float = 1e-4,\n",
    "        interpolator: Union[str, Callable, None] = None,\n",
    "        integral_loss: Union[Callable, None] = None,\n",
    "        seminorm: bool = False,\n",
    "        return_t_eval: bool = True,\n",
    "        optimizable_params: Union[Iterable, Generator] = (),\n",
    "    ):\n",
    "        \"\"\"Generic Neural Ordinary Differential Equation.\n",
    "\n",
    "        Args:\n",
    "            vector_field ([Callable]): the vector field, called with `vector_field(t, x)` for `vector_field(x)`. \n",
    "                                       In the second case, the Callable is automatically wrapped for consistency\n",
    "            solver (Union[str, nn.Module]): \n",
    "            order (int, optional): Order of the ODE. Defaults to 1.\n",
    "            atol (float, optional): Absolute tolerance of the solver. Defaults to 1e-4.\n",
    "            rtol (float, optional): Relative tolerance of the solver. Defaults to 1e-4.\n",
    "            sensitivity (str, optional): Sensitivity method ['autograd', 'adjoint', 'interpolated_adjoint']. Defaults to 'autograd'.\n",
    "            solver_adjoint (Union[str, nn.Module, None], optional): ODE solver for the adjoint. Defaults to None.\n",
    "            atol_adjoint (float, optional): Defaults to 1e-6.\n",
    "            rtol_adjoint (float, optional): Defaults to 1e-6.\n",
    "            integral_loss (Union[Callable, None], optional): Defaults to None.\n",
    "            seminorm (bool, optional): Whether to use seminorms for adaptive stepping in backsolve adjoints. Defaults to False.\n",
    "            return_t_eval (bool): Whether to return (t_eval, sol) or only sol. Useful for chaining NeuralODEs in `nn.Sequential`.\n",
    "            optimizable_parameters (Union[Iterable, Generator]): parameters to calculate sensitivies for. Defaults to ().\n",
    "        Notes:\n",
    "            In `torchdyn`-style, forward calls to a Neural ODE return both a tensor `t_eval` of time points at which the solution is evaluated\n",
    "            as well as the solution itself. This behavior can be controlled by setting `return_t_eval` to False. Calling `trajectory` also returns\n",
    "            the solution only. \n",
    "\n",
    "            The Neural ODE class automates certain delicate steps that must be done depending on the solver and model used. \n",
    "            The `prep_odeint` method carries out such steps. Neural ODEs wrap `ODEProblem`.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            vector_field=standardize_vf_call_signature(\n",
    "                vector_field, order, defunc_wrap=True\n",
    "            ),\n",
    "            order=order,\n",
    "            sensitivity=sensitivity,\n",
    "            solver=solver,\n",
    "            atol=atol,\n",
    "            rtol=rtol,\n",
    "            solver_adjoint=solver_adjoint,\n",
    "            atol_adjoint=atol_adjoint,\n",
    "            rtol_adjoint=rtol_adjoint,\n",
    "            seminorm=seminorm,\n",
    "            interpolator=interpolator,\n",
    "            integral_loss=integral_loss,\n",
    "            optimizable_params=optimizable_params,\n",
    "        )\n",
    "        # data-control conditioning\n",
    "        self._control, self.controlled, self.t_span = None, False, None\n",
    "        self.return_t_eval = return_t_eval\n",
    "        if integral_loss is not None:\n",
    "            self.vf.integral_loss = integral_loss\n",
    "        self.vf.sensitivity = sensitivity\n",
    "\n",
    "    def _prep_integration(self, x: Tensor, t_span: Tensor) -> Tensor:\n",
    "        \"Performs generic checks before integration. Assigns data control inputs and augments state for CNFs\"\n",
    "\n",
    "        # assign a basic value to `t_span` for `forward` calls that do no explicitly pass an integration interval\n",
    "        if t_span is None and self.t_span is None:\n",
    "            t_span = torch.linspace(0, 1, 2)\n",
    "        elif t_span is None:\n",
    "            t_span = self.t_span\n",
    "\n",
    "        # loss dimension detection routine; for CNF div propagation and integral losses w/ autograd\n",
    "        excess_dims = 0\n",
    "        if (not self.integral_loss is None) and self.sensitivity == \"autograd\":\n",
    "            excess_dims += 1\n",
    "\n",
    "        # handle aux. operations required for some jacobian trace CNF estimators e.g Hutchinson's\n",
    "        # as well as datasets-control set to DataControl module\n",
    "        for _, module in self.vf.named_modules():\n",
    "            if hasattr(module, \"trace_estimator\"):\n",
    "                if module.noise_dist is not None:\n",
    "                    module.noise = module.noise_dist.sample((x.shape[0],))\n",
    "                excess_dims += 1\n",
    "\n",
    "            # data-control set routine. Is performed once at the beginning of odeint since the control is fixed to IC\n",
    "            if hasattr(module, \"_control\"):\n",
    "                self.controlled = True\n",
    "                module._control = x[:, excess_dims:].detach()\n",
    "        return x, t_span\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Union[Tensor, Dict],\n",
    "        t_span: Tensor = None,\n",
    "        save_at: Iterable = (),\n",
    "        args={},\n",
    "    ):\n",
    "        x, t_span = self._prep_integration(x, t_span)\n",
    "        t_eval, sol = super().forward(x, t_span, save_at, args)\n",
    "        if self.return_t_eval:\n",
    "            return t_eval, sol\n",
    "        else:\n",
    "            return sol\n",
    "\n",
    "    def trajectory(self, x: torch.Tensor, t_span: Tensor):\n",
    "        x, t_span = self._prep_integration(x, t_span)\n",
    "        _, sol = odeint(\n",
    "            self.vf, x, t_span, solver=self.solver, atol=self.atol, rtol=self.rtol\n",
    "        )\n",
    "        return sol\n",
    "\n",
    "    def __repr__(self):\n",
    "        npar = sum([p.numel() for p in self.vf.parameters()])\n",
    "        return f\"Neural ODE:\\n\\t- order: {self.order}\\\n",
    "        \\n\\t- solver: {self.solver}\\n\\t- adjoint solver: {self.solver_adjoint}\\\n",
    "        \\n\\t- tolerances: relative {self.rtol} absolute {self.atol}\\\n",
    "        \\n\\t- adjoint tolerances: relative {self.rtol_adjoint} absolute {self.atol_adjoint}\\\n",
    "        \\n\\t- num_parameters: {npar}\\\n",
    "        \\n\\t- NFE: {self.vf.nfe}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a368023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25867d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa688ed",
   "metadata": {},
   "source": [
    "## Define some simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4b4fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notice all the f below takes data with 2 dimensions and ouput 2 dimensions (2 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define neural network f:\n",
    "f = nn.Sequential(\n",
    "        nn.Linear(2, 16),\n",
    "        nn.Linear(16, 2),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93a07e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define neural network f:\n",
    "f = nn.Sequential(\n",
    "        nn.Linear(2, 16),\n",
    "        nn.Linear(16, 32),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.Linear(16, 2),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30151478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 128)  # First fully connected layer\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(128, 64)  # Second fully connected layer\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc4 = nn.Linear(32, num_classes) # Output layer\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = self.fc1(xb)\n",
    "        xb = self.bn1(xb)\n",
    "        xb = self.relu1(xb)\n",
    "        xb = self.fc2(xb)\n",
    "        xb = self.bn2(xb)\n",
    "        xb = self.relu2(xb)\n",
    "        xb = self.fc3(xb)\n",
    "        xb = self.relu3(xb)\n",
    "        xb = self.fc4(xb)\n",
    "        return xb\n",
    "\n",
    "f = FCNet(in_features=2, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d146d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3876e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79380f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2c0f716",
   "metadata": {},
   "source": [
    "## Define the model in the cutomized forward and backward flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e290d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "## Model using EPI2 solver\n",
    "model = NeuralODE(f, sensitivity='adjoint', solver=EPI2_one_step(dtype=torch.float32)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa97a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "## Model using EPI2 (adaptive) solver\n",
    "model = NeuralODE(f, sensitivity='adjoint', solver=EPI2_adaptive(dtype=torch.float32)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cca4bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "## Model using DP45 solver\n",
    "model = NeuralODE(f, sensitivity='adjoint', solver=DormandPrince45(dtype=torch.float32)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.012)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bd53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d95ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b8cf61",
   "metadata": {},
   "source": [
    "## Prepare the training and Validation data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd92c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.datasets import *\n",
    "d = ToyDataset()\n",
    "## Prepare the training data loader\n",
    "X, yn = d.generate(n_samples=1024, noise=1e-1, dataset_type='moons')\n",
    "\n",
    "X_train = torch.Tensor(X).to(device)\n",
    "y_train = torch.LongTensor(yn.long()).to(device)\n",
    "train = data.TensorDataset(X_train, y_train)\n",
    "trainloader = data.DataLoader(train, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1576e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the Test data loader\n",
    "X_test, yn_test = d.generate(n_samples=128, noise=1e-1, dataset_type='moons')\n",
    "\n",
    "X_test = torch.Tensor(X_test).to(device)\n",
    "y_test = torch.LongTensor(yn_test.long()).to(device)\n",
    "Test = data.TensorDataset(X_test, y_test)\n",
    "Testloader = data.DataLoader(Test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c157e7d5",
   "metadata": {
    "code_folding": [
     0,
     30
    ]
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total=0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        _, trajectory = model(data, t_span)\n",
    "        output = trajectory[-1]\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate the number of correct predictions\n",
    "        pred = output.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        correct += (pred==target.view_as(pred)).sum().item()\n",
    "\n",
    "        total += target.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    print(f'# of correct in each batch: {correct}')\n",
    "    print(f'# of trials in each batch: {total}')\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Train Epoch: {epoch} Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            _, trajectory = model(data, t_span)\n",
    "            output = trajectory[-1]\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the time span\n",
    "t_span = torch.linspace(0, 1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906061f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57e350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5253310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aac508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "732e57ee",
   "metadata": {},
   "source": [
    "## Start with Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "258c6f6b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_50648\\3522300913.py:56: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 1.276079\n",
      "# of correct in each batch: 197\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 1 Accuracy: 19.24%\n",
      "\n",
      "Test set: Average loss: -0.1451, Accuracy: 24/128 (19%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 1.185476\n",
      "# of correct in each batch: 194\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 2 Accuracy: 18.95%\n",
      "\n",
      "Test set: Average loss: -0.2671, Accuracy: 25/128 (20%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 1.182367\n",
      "# of correct in each batch: 211\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 3 Accuracy: 20.61%\n",
      "\n",
      "Test set: Average loss: -0.3449, Accuracy: 29/128 (23%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 1.048668\n",
      "# of correct in each batch: 245\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 4 Accuracy: 23.93%\n",
      "\n",
      "Test set: Average loss: -0.3870, Accuracy: 33/128 (26%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 1.088594\n",
      "# of correct in each batch: 262\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 5 Accuracy: 25.59%\n",
      "\n",
      "Test set: Average loss: -0.4114, Accuracy: 35/128 (27%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 1.082738\n",
      "# of correct in each batch: 282\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 6 Accuracy: 27.54%\n",
      "\n",
      "Test set: Average loss: -0.4276, Accuracy: 36/128 (28%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 1.112036\n",
      "# of correct in each batch: 293\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 7 Accuracy: 28.61%\n",
      "\n",
      "Test set: Average loss: -0.4405, Accuracy: 38/128 (30%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 1.102015\n",
      "# of correct in each batch: 306\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 8 Accuracy: 29.88%\n",
      "\n",
      "Test set: Average loss: -0.4495, Accuracy: 38/128 (30%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 1.044243\n",
      "# of correct in each batch: 314\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 9 Accuracy: 30.66%\n",
      "\n",
      "Test set: Average loss: -0.4564, Accuracy: 40/128 (31%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 1.082932\n",
      "# of correct in each batch: 319\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 10 Accuracy: 31.15%\n",
      "\n",
      "Test set: Average loss: -0.4615, Accuracy: 40/128 (31%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 1.064049\n",
      "# of correct in each batch: 326\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 11 Accuracy: 31.84%\n",
      "\n",
      "Test set: Average loss: -0.4650, Accuracy: 41/128 (32%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 1.036491\n",
      "# of correct in each batch: 328\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 12 Accuracy: 32.03%\n",
      "\n",
      "Test set: Average loss: -0.4673, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 1.119497\n",
      "# of correct in each batch: 332\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 13 Accuracy: 32.42%\n",
      "\n",
      "Test set: Average loss: -0.4690, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 1.059441\n",
      "# of correct in each batch: 332\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 14 Accuracy: 32.42%\n",
      "\n",
      "Test set: Average loss: -0.4701, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 1.036762\n",
      "# of correct in each batch: 333\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 15 Accuracy: 32.52%\n",
      "\n",
      "Test set: Average loss: -0.4710, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 1.062158\n",
      "# of correct in each batch: 333\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 16 Accuracy: 32.52%\n",
      "\n",
      "Test set: Average loss: -0.4715, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 1.030518\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 17 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4719, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 1.101210\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 18 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4721, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 1.053899\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 19 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4723, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 1.102054\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 20 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4724, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 21 [0/1024 (0%)]\tLoss: 1.059148\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 21 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4725, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 22 [0/1024 (0%)]\tLoss: 1.058139\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 22 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4726, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 23 [0/1024 (0%)]\tLoss: 1.051626\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 23 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4726, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 24 [0/1024 (0%)]\tLoss: 1.087886\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 24 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 25 [0/1024 (0%)]\tLoss: 1.041512\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 25 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 26 [0/1024 (0%)]\tLoss: 1.024014\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 26 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 27 [0/1024 (0%)]\tLoss: 1.051929\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 27 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 28 [0/1024 (0%)]\tLoss: 1.027747\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 28 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 29 [0/1024 (0%)]\tLoss: 1.052474\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 29 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 30 [0/1024 (0%)]\tLoss: 1.029047\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 30 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 31 [0/1024 (0%)]\tLoss: 1.089862\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 31 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 32 [0/1024 (0%)]\tLoss: 1.091214\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 32 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 33 [0/1024 (0%)]\tLoss: 1.107368\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 33 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 34 [0/1024 (0%)]\tLoss: 1.039738\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 34 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 35 [0/1024 (0%)]\tLoss: 1.135226\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 35 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 36 [0/1024 (0%)]\tLoss: 1.017412\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 36 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 37 [0/1024 (0%)]\tLoss: 1.056238\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 37 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 38 [0/1024 (0%)]\tLoss: 1.045976\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 38 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 39 [0/1024 (0%)]\tLoss: 1.064181\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 39 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 40 [0/1024 (0%)]\tLoss: 1.026762\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 40 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 41 [0/1024 (0%)]\tLoss: 1.011368\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 41 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 42 [0/1024 (0%)]\tLoss: 1.032169\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 42 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 43 [0/1024 (0%)]\tLoss: 1.082301\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 43 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 44 [0/1024 (0%)]\tLoss: 1.036345\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 44 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 45 [0/1024 (0%)]\tLoss: 1.055130\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 45 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 46 [0/1024 (0%)]\tLoss: 1.038528\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 46 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 47 [0/1024 (0%)]\tLoss: 1.034389\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 47 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 48 [0/1024 (0%)]\tLoss: 1.063034\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 48 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 49 [0/1024 (0%)]\tLoss: 1.035700\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 49 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 50 [0/1024 (0%)]\tLoss: 1.033203\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 50 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 51 [0/1024 (0%)]\tLoss: 1.124626\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 51 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 52 [0/1024 (0%)]\tLoss: 1.081405\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 52 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 53 [0/1024 (0%)]\tLoss: 1.058342\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 53 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 54 [0/1024 (0%)]\tLoss: 1.055093\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 54 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 55 [0/1024 (0%)]\tLoss: 1.026200\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 55 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 56 [0/1024 (0%)]\tLoss: 1.067552\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 56 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 57 [0/1024 (0%)]\tLoss: 1.058181\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 57 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 58 [0/1024 (0%)]\tLoss: 1.095373\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 58 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 59 [0/1024 (0%)]\tLoss: 1.077345\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 59 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 60 [0/1024 (0%)]\tLoss: 1.055201\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 60 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 61 [0/1024 (0%)]\tLoss: 1.022491\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 61 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 62 [0/1024 (0%)]\tLoss: 1.103317\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 62 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 63 [0/1024 (0%)]\tLoss: 1.035608\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 63 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 64 [0/1024 (0%)]\tLoss: 1.106708\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 64 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 65 [0/1024 (0%)]\tLoss: 1.082313\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 65 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 66 [0/1024 (0%)]\tLoss: 1.041164\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 66 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 67 [0/1024 (0%)]\tLoss: 1.021840\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 67 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 68 [0/1024 (0%)]\tLoss: 1.027960\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 68 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 69 [0/1024 (0%)]\tLoss: 1.146543\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 69 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 70 [0/1024 (0%)]\tLoss: 1.087712\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 70 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 71 [0/1024 (0%)]\tLoss: 1.098279\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 71 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 72 [0/1024 (0%)]\tLoss: 1.050644\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 72 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 73 [0/1024 (0%)]\tLoss: 1.087774\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 73 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 74 [0/1024 (0%)]\tLoss: 1.039271\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 74 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 75 [0/1024 (0%)]\tLoss: 1.068513\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 75 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 76 [0/1024 (0%)]\tLoss: 1.027326\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 76 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 77 [0/1024 (0%)]\tLoss: 1.095221\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 77 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 78 [0/1024 (0%)]\tLoss: 1.045212\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 78 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 79 [0/1024 (0%)]\tLoss: 1.034101\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 79 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 80 [0/1024 (0%)]\tLoss: 1.031362\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 80 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 81 [0/1024 (0%)]\tLoss: 1.110586\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 81 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 82 [0/1024 (0%)]\tLoss: 1.020116\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 82 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 83 [0/1024 (0%)]\tLoss: 1.098415\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 83 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 84 [0/1024 (0%)]\tLoss: 1.108922\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 84 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 85 [0/1024 (0%)]\tLoss: 1.013483\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 85 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 86 [0/1024 (0%)]\tLoss: 1.105826\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 86 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 87 [0/1024 (0%)]\tLoss: 1.103534\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 87 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 88 [0/1024 (0%)]\tLoss: 1.098095\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 88 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 89 [0/1024 (0%)]\tLoss: 1.024496\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 89 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 90 [0/1024 (0%)]\tLoss: 1.088358\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 90 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 91 [0/1024 (0%)]\tLoss: 1.019005\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 91 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 92 [0/1024 (0%)]\tLoss: 1.096813\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 92 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 93 [0/1024 (0%)]\tLoss: 1.030828\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 93 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 94 [0/1024 (0%)]\tLoss: 1.039904\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 94 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 95 [0/1024 (0%)]\tLoss: 1.093028\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 95 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 96 [0/1024 (0%)]\tLoss: 1.108652\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 96 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 97 [0/1024 (0%)]\tLoss: 1.053273\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 97 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 98 [0/1024 (0%)]\tLoss: 1.068728\n",
      "# of correct in each batch: 334\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 98 Accuracy: 32.62%\n",
      "\n",
      "Test set: Average loss: -0.4727, Accuracy: 42/128 (33%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m160\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m         test(model, device, Testloader)\n\u001b[0;32m      4\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      6\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m _, trajectory \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m trajectory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, target)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 105\u001b[0m, in \u001b[0;36mNeuralODE.forward\u001b[1;34m(self, x, t_span, save_at, args)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     99\u001b[0m     x: Union[Tensor, Dict],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     args\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    103\u001b[0m ):\n\u001b[0;32m    104\u001b[0m     x, t_span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_integration(x, t_span)\n\u001b[1;32m--> 105\u001b[0m     t_eval, sol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_t_eval:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t_eval, sol\n",
      "Cell \u001b[1;32mIn[13], line 141\u001b[0m, in \u001b[0;36mODEProblem.forward\u001b[1;34m(self, x, t_span, save_at, args)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, t_span: Tensor, save_at: Tensor \u001b[38;5;241m=\u001b[39m (), args\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor safety redirects to intended method `odeint`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 137\u001b[0m, in \u001b[0;36mODEProblem.odeint\u001b[1;34m(self, x, t_span, save_at, args)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m odeint(\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf,\n\u001b[0;32m    127\u001b[0m         x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograd_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36m_gather_odefunc_adjoint.<locals>._ODEProblemFunc.forward\u001b[1;34m(ctx, vf_params, x, t_span, B, save_at)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, vf_params, x, t_span, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, save_at\u001b[38;5;241m=\u001b[39m()):\n\u001b[1;32m---> 20\u001b[0m     t_sol, sol \u001b[38;5;241m=\u001b[39m \u001b[43mgeneric_odeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(sol, t_sol)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t_sol, sol\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mgeneric_odeint\u001b[1;34m(problem_type, vf, x, t_span, solver, atol, rtol, interpolator, B0, return_all_eval, maxiter, fine_steps, save_at)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDispatches to appropriate `odeint` function depending on `Problem` class (ODEProblem, MultipleShootingProblem)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_all_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msave_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_shooting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m odeint_mshooting(vf, x, t_span, solver, B0\u001b[38;5;241m=\u001b[39mB0, fine_steps\u001b[38;5;241m=\u001b[39mfine_steps, maxiter\u001b[38;5;241m=\u001b[39mmaxiter)\n",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m, in \u001b[0;36modeint\u001b[1;34m(f, x, t_span, solver, atol, rtol, t_stops, verbose, interpolator, return_all_eval, save_at, args, seminorm)\u001b[0m\n\u001b[0;32m     56\u001b[0m         warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting tolerances has no effect on fixed-step methods\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# instantiate save_at tensor\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fixed_odeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m stepping_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     60\u001b[0m     t \u001b[38;5;241m=\u001b[39m t_span[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36m_fixed_odeint\u001b[1;34m(f, x, t_span, solver, save_at, args)\u001b[0m\n\u001b[0;32m     17\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(t_span) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     _, x, _,_ \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     t \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m dt\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misclose(t, save_at)\u001b[38;5;241m.\u001b[39msum():\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mEPI2_one_step.step\u001b[1;34m(self, f, x_n, t, dt, k1, args)\u001b[0m\n\u001b[0;32m     20\u001b[0m     _, A_x_product \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjvp(\u001b[38;5;28;01mlambda\u001b[39;00m x: f(t, x), x, x) \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A_x_product \u001b[38;5;241m+\u001b[39m constants\n\u001b[1;32m---> 23\u001b[0m _, increment, _, _\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43minit_cond_epi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m x_sol\u001b[38;5;241m=\u001b[39mx_n\u001b[38;5;241m+\u001b[39mincrement\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, x_sol, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mDormandPrince45.step\u001b[1;34m(self, f, x, t, dt, k1, args)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, x, t, dt, k1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     10\u001b[0m     c, a, bsol, berr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtableau\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: k1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m f(t \u001b[38;5;241m+\u001b[39m c[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1)\n\u001b[0;32m     13\u001b[0m     k3 \u001b[38;5;241m=\u001b[39m f(t \u001b[38;5;241m+\u001b[39m c[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m (a[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m a[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m k2))\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mEPI2_one_step.step.<locals>.f_combined\u001b[1;34m(t, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m f_x_t \u001b[38;5;241m=\u001b[39m f(t, x)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the Jacobian-vector product (A_n * x) using torch.autograd.functional.jacobian\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m _, A_x_product \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A_x_product \u001b[38;5;241m+\u001b[39m constants\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\functional.py:434\u001b[0m, in \u001b[0;36mjvp\u001b[1;34m(func, inputs, v, create_graph, strict)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnelement() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    429\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vector v can only be None if the input to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe user-provided function is a single Tensor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a single element.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         )\n\u001b[1;32m--> 434\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[0;32m    436\u001b[0m     outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjvp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m )\n\u001b[0;32m    438\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mEPI2_one_step.step.<locals>.f_combined.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m f_x_t \u001b[38;5;241m=\u001b[39m f(t, x)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the Jacobian-vector product (A_n * x) using torch.autograd.functional.jacobian\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m _, A_x_product \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjvp(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, x, x) \n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A_x_product \u001b[38;5;241m+\u001b[39m constants\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m, in \u001b[0;36mDEFunc.forward\u001b[1;34m(self, t, x, args)\u001b[0m\n\u001b[0;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigher_order_forward(t, x)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m, in \u001b[0;36mDEFuncBase.forward\u001b[1;34m(self, t, x, args)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf(t, x, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## simple model with EPI2 solver:\n",
    "for epoch in range(1, 160+1):\n",
    "        train(model, device, trainloader, optimizer, epoch)\n",
    "        test(model, device, Testloader)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0d905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "621ab9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_50648\\3522300913.py:56: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 1.315729\n",
      "# of correct in each batch: 205\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 1 Accuracy: 20.02%\n",
      "\n",
      "Test set: Average loss: -0.6464, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 0.990297\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 2 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -0.9326, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 0.940363\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 3 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -1.0374, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 0.909150\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 4 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -1.2826, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 0.778094\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 5 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -1.3310, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 0.771000\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 6 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -1.3016, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 0.740356\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 7 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -1.2157, Accuracy: 63/128 (49%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 0.690455\n",
      "# of correct in each batch: 506\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 8 Accuracy: 49.41%\n",
      "\n",
      "Test set: Average loss: -1.1286, Accuracy: 64/128 (50%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 0.719183\n",
      "# of correct in each batch: 516\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 9 Accuracy: 50.39%\n",
      "\n",
      "Test set: Average loss: -1.0911, Accuracy: 67/128 (52%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 0.705930\n",
      "# of correct in each batch: 540\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 10 Accuracy: 52.73%\n",
      "\n",
      "Test set: Average loss: -1.0790, Accuracy: 69/128 (54%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 0.726415\n",
      "# of correct in each batch: 548\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 11 Accuracy: 53.52%\n",
      "\n",
      "Test set: Average loss: -1.0765, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 0.712156\n",
      "# of correct in each batch: 556\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 12 Accuracy: 54.30%\n",
      "\n",
      "Test set: Average loss: -1.0786, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 0.714949\n",
      "# of correct in each batch: 560\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 13 Accuracy: 54.69%\n",
      "\n",
      "Test set: Average loss: -1.0797, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 0.719253\n",
      "# of correct in each batch: 560\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 14 Accuracy: 54.69%\n",
      "\n",
      "Test set: Average loss: -1.0822, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 0.717849\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 15 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0840, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 0.719549\n",
      "# of correct in each batch: 562\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 16 Accuracy: 54.88%\n",
      "\n",
      "Test set: Average loss: -1.0852, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 0.696196\n",
      "# of correct in each batch: 562\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 17 Accuracy: 54.88%\n",
      "\n",
      "Test set: Average loss: -1.0863, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 0.715650\n",
      "# of correct in each batch: 562\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 18 Accuracy: 54.88%\n",
      "\n",
      "Test set: Average loss: -1.0873, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 0.723618\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 19 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0879, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 0.723106\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 20 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0884, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 21 [0/1024 (0%)]\tLoss: 0.714715\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 21 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0885, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 22 [0/1024 (0%)]\tLoss: 0.711531\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 22 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0888, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 23 [0/1024 (0%)]\tLoss: 0.709717\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 23 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0894, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 24 [0/1024 (0%)]\tLoss: 0.703412\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 24 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0894, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 25 [0/1024 (0%)]\tLoss: 0.697976\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 25 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 26 [0/1024 (0%)]\tLoss: 0.699312\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 26 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 27 [0/1024 (0%)]\tLoss: 0.693326\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 27 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 28 [0/1024 (0%)]\tLoss: 0.702396\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 28 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 29 [0/1024 (0%)]\tLoss: 0.714549\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 29 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 30 [0/1024 (0%)]\tLoss: 0.701995\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 30 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 31 [0/1024 (0%)]\tLoss: 0.709143\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 31 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 32 [0/1024 (0%)]\tLoss: 0.714583\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 32 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 33 [0/1024 (0%)]\tLoss: 0.706320\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 33 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0895, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 34 [0/1024 (0%)]\tLoss: 0.716287\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 34 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 35 [0/1024 (0%)]\tLoss: 0.722952\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 35 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 36 [0/1024 (0%)]\tLoss: 0.708872\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 36 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 37 [0/1024 (0%)]\tLoss: 0.707554\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 37 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 38 [0/1024 (0%)]\tLoss: 0.713759\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 38 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 39 [0/1024 (0%)]\tLoss: 0.709092\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 39 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 40 [0/1024 (0%)]\tLoss: 0.705547\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 40 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 41 [0/1024 (0%)]\tLoss: 0.695822\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 41 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 42 [0/1024 (0%)]\tLoss: 0.711248\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 42 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 43 [0/1024 (0%)]\tLoss: 0.713405\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 43 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 44 [0/1024 (0%)]\tLoss: 0.713274\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 44 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 45 [0/1024 (0%)]\tLoss: 0.710505\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 45 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 46 [0/1024 (0%)]\tLoss: 0.725555\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 46 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 47 [0/1024 (0%)]\tLoss: 0.707719\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 47 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 48 [0/1024 (0%)]\tLoss: 0.703609\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 48 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 49 [0/1024 (0%)]\tLoss: 0.701371\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 49 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 50 [0/1024 (0%)]\tLoss: 0.699517\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 50 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 51 [0/1024 (0%)]\tLoss: 0.704573\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 51 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 52 [0/1024 (0%)]\tLoss: 0.700421\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 52 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 53 [0/1024 (0%)]\tLoss: 0.703697\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 53 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 54 [0/1024 (0%)]\tLoss: 0.725333\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 54 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 55 [0/1024 (0%)]\tLoss: 0.705783\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 55 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 56 [0/1024 (0%)]\tLoss: 0.694297\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 56 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 57 [0/1024 (0%)]\tLoss: 0.681349\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 57 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 58 [0/1024 (0%)]\tLoss: 0.723936\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 58 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 59 [0/1024 (0%)]\tLoss: 0.716875\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 59 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 60 [0/1024 (0%)]\tLoss: 0.705508\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 60 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 61 [0/1024 (0%)]\tLoss: 0.707942\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 61 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 62 [0/1024 (0%)]\tLoss: 0.706902\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 62 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 63 [0/1024 (0%)]\tLoss: 0.703557\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 63 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 64 [0/1024 (0%)]\tLoss: 0.692599\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 64 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 65 [0/1024 (0%)]\tLoss: 0.694514\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 65 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 66 [0/1024 (0%)]\tLoss: 0.716422\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 66 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 67 [0/1024 (0%)]\tLoss: 0.718681\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 67 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 68 [0/1024 (0%)]\tLoss: 0.726360\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 68 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 69 [0/1024 (0%)]\tLoss: 0.720134\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 69 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 70 [0/1024 (0%)]\tLoss: 0.707899\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 70 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 71 [0/1024 (0%)]\tLoss: 0.711123\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 71 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 72 [0/1024 (0%)]\tLoss: 0.709295\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 72 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 73 [0/1024 (0%)]\tLoss: 0.701558\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 73 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 74 [0/1024 (0%)]\tLoss: 0.711183\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 74 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 75 [0/1024 (0%)]\tLoss: 0.722421\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 75 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 76 [0/1024 (0%)]\tLoss: 0.703141\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 76 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 77 [0/1024 (0%)]\tLoss: 0.706473\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 77 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 78 [0/1024 (0%)]\tLoss: 0.700380\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 78 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 79 [0/1024 (0%)]\tLoss: 0.706939\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 79 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 80 [0/1024 (0%)]\tLoss: 0.709528\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 80 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 81 [0/1024 (0%)]\tLoss: 0.707394\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 81 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 82 [0/1024 (0%)]\tLoss: 0.726364\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 82 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 83 [0/1024 (0%)]\tLoss: 0.719796\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 83 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 84 [0/1024 (0%)]\tLoss: 0.715809\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 84 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 85 [0/1024 (0%)]\tLoss: 0.689590\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 85 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 86 [0/1024 (0%)]\tLoss: 0.706630\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 86 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 87 [0/1024 (0%)]\tLoss: 0.702735\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 87 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 88 [0/1024 (0%)]\tLoss: 0.714839\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 88 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 89 [0/1024 (0%)]\tLoss: 0.722653\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 89 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 90 [0/1024 (0%)]\tLoss: 0.716524\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 90 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 91 [0/1024 (0%)]\tLoss: 0.714371\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 91 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 92 [0/1024 (0%)]\tLoss: 0.715097\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 92 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 93 [0/1024 (0%)]\tLoss: 0.693631\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 93 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 94 [0/1024 (0%)]\tLoss: 0.698323\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 94 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 95 [0/1024 (0%)]\tLoss: 0.700071\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 95 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 96 [0/1024 (0%)]\tLoss: 0.704361\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 96 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 97 [0/1024 (0%)]\tLoss: 0.694757\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 97 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 98 [0/1024 (0%)]\tLoss: 0.733973\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 98 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 99 [0/1024 (0%)]\tLoss: 0.710832\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 99 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 100 [0/1024 (0%)]\tLoss: 0.701052\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 100 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 101 [0/1024 (0%)]\tLoss: 0.710549\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 101 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 102 [0/1024 (0%)]\tLoss: 0.703624\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 102 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 103 [0/1024 (0%)]\tLoss: 0.719434\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 103 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 104 [0/1024 (0%)]\tLoss: 0.715171\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 104 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 105 [0/1024 (0%)]\tLoss: 0.698779\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 105 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 106 [0/1024 (0%)]\tLoss: 0.720199\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 106 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 107 [0/1024 (0%)]\tLoss: 0.729239\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 107 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 108 [0/1024 (0%)]\tLoss: 0.728542\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 108 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 109 [0/1024 (0%)]\tLoss: 0.704513\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 109 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 110 [0/1024 (0%)]\tLoss: 0.711065\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 110 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 111 [0/1024 (0%)]\tLoss: 0.703883\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 111 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 112 [0/1024 (0%)]\tLoss: 0.714316\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 112 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 113 [0/1024 (0%)]\tLoss: 0.717725\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 113 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 114 [0/1024 (0%)]\tLoss: 0.724134\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 114 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 115 [0/1024 (0%)]\tLoss: 0.713550\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 115 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 116 [0/1024 (0%)]\tLoss: 0.725533\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 116 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 117 [0/1024 (0%)]\tLoss: 0.698795\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 117 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 118 [0/1024 (0%)]\tLoss: 0.704783\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 118 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 119 [0/1024 (0%)]\tLoss: 0.706796\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 119 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n",
      "Train Epoch: 120 [0/1024 (0%)]\tLoss: 0.724197\n",
      "# of correct in each batch: 561\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 120 Accuracy: 54.79%\n",
      "\n",
      "Test set: Average loss: -1.0896, Accuracy: 70/128 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## complex f with EPI2 solver:\n",
    "for epoch in range(1, 120+1):\n",
    "        train(model, device, trainloader, optimizer, epoch)\n",
    "        test(model, device, Testloader)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14d589d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_50648\\3522300913.py:56: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 1.306714\n",
      "# of correct in each batch: 169\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 1 Accuracy: 16.50%\n",
      "\n",
      "Test set: Average loss: -0.9219, Accuracy: 26/128 (20%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 0.841029\n",
      "# of correct in each batch: 432\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 2 Accuracy: 42.19%\n",
      "\n",
      "Test set: Average loss: -1.8157, Accuracy: 72/128 (56%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 0.676446\n",
      "# of correct in each batch: 557\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 3 Accuracy: 54.39%\n",
      "\n",
      "Test set: Average loss: -1.9957, Accuracy: 66/128 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 0.654738\n",
      "# of correct in each batch: 545\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 4 Accuracy: 53.22%\n",
      "\n",
      "Test set: Average loss: -2.2955, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 0.629669\n",
      "# of correct in each batch: 600\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 5 Accuracy: 58.59%\n",
      "\n",
      "Test set: Average loss: -2.7562, Accuracy: 84/128 (66%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 0.618452\n",
      "# of correct in each batch: 687\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 6 Accuracy: 67.09%\n",
      "\n",
      "Test set: Average loss: -3.1630, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 0.592046\n",
      "# of correct in each batch: 742\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 7 Accuracy: 72.46%\n",
      "\n",
      "Test set: Average loss: -3.4571, Accuracy: 93/128 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 0.602330\n",
      "# of correct in each batch: 739\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 8 Accuracy: 72.17%\n",
      "\n",
      "Test set: Average loss: -3.5257, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 0.609717\n",
      "# of correct in each batch: 701\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 9 Accuracy: 68.46%\n",
      "\n",
      "Test set: Average loss: -3.5787, Accuracy: 86/128 (67%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 0.575419\n",
      "# of correct in each batch: 702\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 10 Accuracy: 68.55%\n",
      "\n",
      "Test set: Average loss: -3.6997, Accuracy: 86/128 (67%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 0.557043\n",
      "# of correct in each batch: 703\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 11 Accuracy: 68.65%\n",
      "\n",
      "Test set: Average loss: -3.8486, Accuracy: 87/128 (68%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 0.552739\n",
      "# of correct in each batch: 712\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 12 Accuracy: 69.53%\n",
      "\n",
      "Test set: Average loss: -3.9688, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 0.569936\n",
      "# of correct in each batch: 715\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 13 Accuracy: 69.82%\n",
      "\n",
      "Test set: Average loss: -4.0574, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 0.571236\n",
      "# of correct in each batch: 714\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 14 Accuracy: 69.73%\n",
      "\n",
      "Test set: Average loss: -4.1197, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 0.556909\n",
      "# of correct in each batch: 716\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 15 Accuracy: 69.92%\n",
      "\n",
      "Test set: Average loss: -4.1610, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 0.535475\n",
      "# of correct in each batch: 715\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 16 Accuracy: 69.82%\n",
      "\n",
      "Test set: Average loss: -4.1893, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 0.528743\n",
      "# of correct in each batch: 716\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 17 Accuracy: 69.92%\n",
      "\n",
      "Test set: Average loss: -4.2071, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 0.531339\n",
      "# of correct in each batch: 716\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 18 Accuracy: 69.92%\n",
      "\n",
      "Test set: Average loss: -4.2191, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 0.577561\n",
      "# of correct in each batch: 716\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 19 Accuracy: 69.92%\n",
      "\n",
      "Test set: Average loss: -4.2259, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 0.551134\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 20 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2318, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 21 [0/1024 (0%)]\tLoss: 0.588697\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 21 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2357, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 22 [0/1024 (0%)]\tLoss: 0.520169\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 22 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2389, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 23 [0/1024 (0%)]\tLoss: 0.539197\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 23 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2409, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 24 [0/1024 (0%)]\tLoss: 0.560026\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 24 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2422, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 25 [0/1024 (0%)]\tLoss: 0.552300\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 25 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2432, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 26 [0/1024 (0%)]\tLoss: 0.565754\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 26 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2438, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 27 [0/1024 (0%)]\tLoss: 0.543791\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 27 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2443, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 28 [0/1024 (0%)]\tLoss: 0.545282\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 28 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2447, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 29 [0/1024 (0%)]\tLoss: 0.574590\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 29 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2449, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 30 [0/1024 (0%)]\tLoss: 0.575381\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 30 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2451, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 31 [0/1024 (0%)]\tLoss: 0.574284\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 31 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2452, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 32 [0/1024 (0%)]\tLoss: 0.579854\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 32 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2453, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 33 [0/1024 (0%)]\tLoss: 0.556002\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 33 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2453, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 34 [0/1024 (0%)]\tLoss: 0.560189\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 34 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2454, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 35 [0/1024 (0%)]\tLoss: 0.533722\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 35 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2454, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 36 [0/1024 (0%)]\tLoss: 0.556411\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 36 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2454, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 37 [0/1024 (0%)]\tLoss: 0.554169\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 37 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2454, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 38 [0/1024 (0%)]\tLoss: 0.563304\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 38 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 39 [0/1024 (0%)]\tLoss: 0.555994\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 39 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 40 [0/1024 (0%)]\tLoss: 0.580352\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 40 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 41 [0/1024 (0%)]\tLoss: 0.579891\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 41 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 42 [0/1024 (0%)]\tLoss: 0.548822\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 42 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 43 [0/1024 (0%)]\tLoss: 0.553519\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 43 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 44 [0/1024 (0%)]\tLoss: 0.568548\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 44 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 45 [0/1024 (0%)]\tLoss: 0.570564\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 45 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 46 [0/1024 (0%)]\tLoss: 0.556343\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 46 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 47 [0/1024 (0%)]\tLoss: 0.575958\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 47 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 48 [0/1024 (0%)]\tLoss: 0.560402\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 48 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 49 [0/1024 (0%)]\tLoss: 0.567645\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 49 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 50 [0/1024 (0%)]\tLoss: 0.587115\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 50 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 51 [0/1024 (0%)]\tLoss: 0.576475\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 51 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 52 [0/1024 (0%)]\tLoss: 0.568606\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 52 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 53 [0/1024 (0%)]\tLoss: 0.536409\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 53 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 54 [0/1024 (0%)]\tLoss: 0.575567\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 54 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 55 [0/1024 (0%)]\tLoss: 0.579328\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 55 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 56 [0/1024 (0%)]\tLoss: 0.557160\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 56 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 57 [0/1024 (0%)]\tLoss: 0.546325\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 57 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 58 [0/1024 (0%)]\tLoss: 0.562057\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 58 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 59 [0/1024 (0%)]\tLoss: 0.562750\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 59 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 60 [0/1024 (0%)]\tLoss: 0.535196\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 60 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 61 [0/1024 (0%)]\tLoss: 0.581269\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 61 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 62 [0/1024 (0%)]\tLoss: 0.554342\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 62 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 63 [0/1024 (0%)]\tLoss: 0.564500\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 63 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 64 [0/1024 (0%)]\tLoss: 0.551592\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 64 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 65 [0/1024 (0%)]\tLoss: 0.553305\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 65 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 66 [0/1024 (0%)]\tLoss: 0.569535\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 66 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 67 [0/1024 (0%)]\tLoss: 0.536181\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 67 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 68 [0/1024 (0%)]\tLoss: 0.572644\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 68 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 69 [0/1024 (0%)]\tLoss: 0.546824\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 69 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 70 [0/1024 (0%)]\tLoss: 0.528732\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 70 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 71 [0/1024 (0%)]\tLoss: 0.545129\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 71 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 72 [0/1024 (0%)]\tLoss: 0.555009\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 72 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 73 [0/1024 (0%)]\tLoss: 0.562478\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 73 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 74 [0/1024 (0%)]\tLoss: 0.563834\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 74 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 75 [0/1024 (0%)]\tLoss: 0.580402\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 75 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 76 [0/1024 (0%)]\tLoss: 0.578172\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 76 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 77 [0/1024 (0%)]\tLoss: 0.545159\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 77 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 78 [0/1024 (0%)]\tLoss: 0.550669\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 78 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 79 [0/1024 (0%)]\tLoss: 0.564920\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 79 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 80 [0/1024 (0%)]\tLoss: 0.548726\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 80 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 81 [0/1024 (0%)]\tLoss: 0.553458\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 81 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 82 [0/1024 (0%)]\tLoss: 0.566415\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 82 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 83 [0/1024 (0%)]\tLoss: 0.583970\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 83 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 84 [0/1024 (0%)]\tLoss: 0.572935\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 84 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 85 [0/1024 (0%)]\tLoss: 0.557013\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 85 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 86 [0/1024 (0%)]\tLoss: 0.538274\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 86 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 87 [0/1024 (0%)]\tLoss: 0.588097\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 87 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 88 [0/1024 (0%)]\tLoss: 0.543784\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 88 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 89 [0/1024 (0%)]\tLoss: 0.585223\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 89 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 90 [0/1024 (0%)]\tLoss: 0.556726\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 90 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 91 [0/1024 (0%)]\tLoss: 0.544042\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 91 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 92 [0/1024 (0%)]\tLoss: 0.586770\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 92 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 93 [0/1024 (0%)]\tLoss: 0.548720\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 93 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 94 [0/1024 (0%)]\tLoss: 0.569838\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 94 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 95 [0/1024 (0%)]\tLoss: 0.527009\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 95 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 96 [0/1024 (0%)]\tLoss: 0.514662\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 96 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 97 [0/1024 (0%)]\tLoss: 0.544603\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 97 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 98 [0/1024 (0%)]\tLoss: 0.556004\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 98 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 99 [0/1024 (0%)]\tLoss: 0.591637\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 99 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 100 [0/1024 (0%)]\tLoss: 0.557864\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 100 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 101 [0/1024 (0%)]\tLoss: 0.561417\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 101 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 102 [0/1024 (0%)]\tLoss: 0.550449\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 102 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 103 [0/1024 (0%)]\tLoss: 0.556033\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 103 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 104 [0/1024 (0%)]\tLoss: 0.533767\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 104 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 105 [0/1024 (0%)]\tLoss: 0.563482\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 105 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 106 [0/1024 (0%)]\tLoss: 0.538094\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 106 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 107 [0/1024 (0%)]\tLoss: 0.552961\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 107 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 108 [0/1024 (0%)]\tLoss: 0.567483\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 108 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 109 [0/1024 (0%)]\tLoss: 0.566954\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 109 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 110 [0/1024 (0%)]\tLoss: 0.577390\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 110 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 111 [0/1024 (0%)]\tLoss: 0.540130\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 111 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 112 [0/1024 (0%)]\tLoss: 0.565902\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 112 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 113 [0/1024 (0%)]\tLoss: 0.542595\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 113 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 114 [0/1024 (0%)]\tLoss: 0.575303\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 114 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 115 [0/1024 (0%)]\tLoss: 0.548801\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 115 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 116 [0/1024 (0%)]\tLoss: 0.562886\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 116 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 117 [0/1024 (0%)]\tLoss: 0.563089\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 117 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 118 [0/1024 (0%)]\tLoss: 0.566946\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 118 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 119 [0/1024 (0%)]\tLoss: 0.570965\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 119 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n",
      "Train Epoch: 120 [0/1024 (0%)]\tLoss: 0.557435\n",
      "# of correct in each batch: 717\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 120 Accuracy: 70.02%\n",
      "\n",
      "Test set: Average loss: -4.2455, Accuracy: 89/128 (70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## complex f with EPI2 solver:\n",
    "for epoch in range(1, 120+1):\n",
    "        train(model, device, trainloader, optimizer, epoch)\n",
    "        test(model, device, Testloader)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c5188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f2626b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_50648\\3522300913.py:56: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 1.307709\n",
      "# of correct in each batch: 356\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 1 Accuracy: 34.77%\n",
      "\n",
      "Test set: Average loss: -1.2051, Accuracy: 72/128 (56%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 0.688071\n",
      "# of correct in each batch: 614\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 2 Accuracy: 59.96%\n",
      "\n",
      "Test set: Average loss: -3.7382, Accuracy: 79/128 (62%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 0.627000\n",
      "# of correct in each batch: 670\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 3 Accuracy: 65.43%\n",
      "\n",
      "Test set: Average loss: -29.3953, Accuracy: 81/128 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 0.606214\n",
      "# of correct in each batch: 630\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 4 Accuracy: 61.52%\n",
      "\n",
      "Test set: Average loss: -122.1381, Accuracy: 79/128 (62%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 0.578914\n",
      "# of correct in each batch: 619\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 5 Accuracy: 60.45%\n",
      "\n",
      "Test set: Average loss: -590.3235, Accuracy: 76/128 (59%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 0.563546\n",
      "# of correct in each batch: 624\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 6 Accuracy: 60.94%\n",
      "\n",
      "Test set: Average loss: -863.5206, Accuracy: 75/128 (59%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 0.551045\n",
      "# of correct in each batch: 610\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 7 Accuracy: 59.57%\n",
      "\n",
      "Test set: Average loss: -674.9303, Accuracy: 74/128 (58%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 0.598533\n",
      "# of correct in each batch: 604\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 8 Accuracy: 58.98%\n",
      "\n",
      "Test set: Average loss: -716.7100, Accuracy: 76/128 (59%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 0.595997\n",
      "# of correct in each batch: 615\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 9 Accuracy: 60.06%\n",
      "\n",
      "Test set: Average loss: -1086.0920, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 0.589421\n",
      "# of correct in each batch: 619\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 10 Accuracy: 60.45%\n",
      "\n",
      "Test set: Average loss: -1214.2433, Accuracy: 74/128 (58%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 0.563793\n",
      "# of correct in each batch: 617\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 11 Accuracy: 60.25%\n",
      "\n",
      "Test set: Average loss: -1233.9590, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 0.541831\n",
      "# of correct in each batch: 623\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 12 Accuracy: 60.84%\n",
      "\n",
      "Test set: Average loss: -1812.9929, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 0.538270\n",
      "# of correct in each batch: 622\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 13 Accuracy: 60.74%\n",
      "\n",
      "Test set: Average loss: -1415.4906, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 0.551246\n",
      "# of correct in each batch: 629\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 14 Accuracy: 61.43%\n",
      "\n",
      "Test set: Average loss: -1380.1907, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 0.528655\n",
      "# of correct in each batch: 616\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 15 Accuracy: 60.16%\n",
      "\n",
      "Test set: Average loss: -1272.6359, Accuracy: 74/128 (58%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 0.592115\n",
      "# of correct in each batch: 621\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 16 Accuracy: 60.64%\n",
      "\n",
      "Test set: Average loss: -2124.0354, Accuracy: 74/128 (58%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 0.559157\n",
      "# of correct in each batch: 623\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 17 Accuracy: 60.84%\n",
      "\n",
      "Test set: Average loss: -2233.2742, Accuracy: 73/128 (57%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 0.554564\n",
      "# of correct in each batch: 626\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 18 Accuracy: 61.13%\n",
      "\n",
      "Test set: Average loss: -2664.2734, Accuracy: 74/128 (58%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 0.540915\n",
      "# of correct in each batch: 629\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 19 Accuracy: 61.43%\n",
      "\n",
      "Test set: Average loss: -1814.9210, Accuracy: 71/128 (55%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 0.609973\n",
      "# of correct in each batch: 616\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 20 Accuracy: 60.16%\n",
      "\n",
      "Test set: Average loss: -1817.9689, Accuracy: 73/128 (57%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## complex f with EPI2 solver:\n",
    "for epoch in range(1, 20+1):\n",
    "        train(model, device, trainloader, optimizer, epoch)\n",
    "        test(model, device, Testloader)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b836814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_50648\\3522300913.py:56: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 1.260444\n",
      "# of correct in each batch: 388\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 1 Accuracy: 37.89%\n",
      "\n",
      "Test set: Average loss: -0.2897, Accuracy: 35/128 (27%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 0.707728\n",
      "# of correct in each batch: 505\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 2 Accuracy: 49.32%\n",
      "\n",
      "Test set: Average loss: -0.2464, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 0.695036\n",
      "# of correct in each batch: 507\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 3 Accuracy: 49.51%\n",
      "\n",
      "Test set: Average loss: -0.2519, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 0.708746\n",
      "# of correct in each batch: 492\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 4 Accuracy: 48.05%\n",
      "\n",
      "Test set: Average loss: -0.2413, Accuracy: 41/128 (32%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 0.688592\n",
      "# of correct in each batch: 570\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 5 Accuracy: 55.66%\n",
      "\n",
      "Test set: Average loss: -0.2146, Accuracy: 39/128 (30%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 0.729913\n",
      "# of correct in each batch: 469\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 6 Accuracy: 45.80%\n",
      "\n",
      "Test set: Average loss: -0.1825, Accuracy: 41/128 (32%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 0.771502\n",
      "# of correct in each batch: 472\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 7 Accuracy: 46.09%\n",
      "\n",
      "Test set: Average loss: -0.1519, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 0.754317\n",
      "# of correct in each batch: 472\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 8 Accuracy: 46.09%\n",
      "\n",
      "Test set: Average loss: -0.1612, Accuracy: 44/128 (34%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 0.693794\n",
      "# of correct in each batch: 493\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 9 Accuracy: 48.14%\n",
      "\n",
      "Test set: Average loss: -0.1232, Accuracy: 44/128 (34%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 0.680319\n",
      "# of correct in each batch: 489\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 10 Accuracy: 47.75%\n",
      "\n",
      "Test set: Average loss: -0.2307, Accuracy: 42/128 (33%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 0.693914\n",
      "# of correct in each batch: 488\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 11 Accuracy: 47.66%\n",
      "\n",
      "Test set: Average loss: -0.1845, Accuracy: 44/128 (34%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 0.662213\n",
      "# of correct in each batch: 512\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 12 Accuracy: 50.00%\n",
      "\n",
      "Test set: Average loss: -0.1800, Accuracy: 45/128 (35%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 0.681989\n",
      "# of correct in each batch: 507\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 13 Accuracy: 49.51%\n",
      "\n",
      "Test set: Average loss: -0.1980, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 0.682765\n",
      "# of correct in each batch: 464\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 14 Accuracy: 45.31%\n",
      "\n",
      "Test set: Average loss: -0.1882, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 0.666423\n",
      "# of correct in each batch: 499\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 15 Accuracy: 48.73%\n",
      "\n",
      "Test set: Average loss: -0.1771, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 0.677551\n",
      "# of correct in each batch: 509\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 16 Accuracy: 49.71%\n",
      "\n",
      "Test set: Average loss: -0.1628, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 0.681367\n",
      "# of correct in each batch: 526\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 17 Accuracy: 51.37%\n",
      "\n",
      "Test set: Average loss: -0.1499, Accuracy: 41/128 (32%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 0.669171\n",
      "# of correct in each batch: 508\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 18 Accuracy: 49.61%\n",
      "\n",
      "Test set: Average loss: -0.1774, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 0.675279\n",
      "# of correct in each batch: 517\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 19 Accuracy: 50.49%\n",
      "\n",
      "Test set: Average loss: -0.2053, Accuracy: 43/128 (34%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 0.680129\n",
      "# of correct in each batch: 496\n",
      "# of trials in each batch: 1024\n",
      "Train Epoch: 20 Accuracy: 48.44%\n",
      "\n",
      "Test set: Average loss: -0.1737, Accuracy: 41/128 (32%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## complex f with EPI2 solver:\n",
    "for epoch in range(1, 20+1):\n",
    "        train(model, device, trainloader, optimizer, epoch)\n",
    "        test(model, device, Testloader)\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
