---
layout: page
title: "Clash-Royale AI agent ğŸ°âš”ï¸"
permalink: /projects/project2.html
---

## AI Agent Visualization ğŸ®ğŸ¤–âœ¨

## ğŸ”¬ Brief Project Flow ğŸ“

Since we have no access to the internal control for Clash Royale ğŸ°, so that no direct information to extract state, action, and reward for traditional online training in Reinforcement Learning. We have to build the visual detection block ğŸ‘€ to extract the necessary information for training data. 

1. **Dataset/Episode Collection ğŸ“¸**
   - 1.1 Use [Scrcpy](https://github.com/Genymobile/scrcpy) and **FFmpeg** ğŸ¬ to access the Android phone screen and extract the game episode frames (later add the link for the dataset collection). Notice we have to use the Expert datasets ğŸ†.
   
   - 1.2 Prepare troops segments ğŸ¤–ğŸ›¡ï¸ for generative arena images, which is the training dataset for visual detection model YOLOv8:
  
   ![Generative Arena View](generative_arena.png)
     
3. **Visual Detection/ Data Processing ğŸ”**
   - 2.1 **State builder** (torch_state_builder.py): Use trained YOLOv8 ğŸ¶ for arena information extraction to build state **s**, use ResNet classification ğŸ–¼ï¸ to extract info for deployed cards and elixir ğŸ’§, use cnocr ğŸ”¤ to extract actual time â±ï¸.
   
   - 2.2 **Action builder** (torch_action_builder.py): Use ResNet classification ğŸ–¼ï¸ to visually detect card deployments ğŸƒ.
   
   - 2.3 **Reward builder** (torch_reward_builder.py): Use cnocr ğŸ”¤ to detect queen ğŸ‘‘ and king ğŸ° tower HP deduction ğŸ’”.
   
   - 2.4 Every frame would have the corresponding state (s) ğŸŸ¢, action (a) âš¡, and reward (r) â­

4. **Policy Network Construction ğŸ§ **
   
 ![Policy Network Preview](policy_model_en.png)

6. **AWS Sagemaker Training â˜ï¸**

## Training Dataset Construction ğŸ—‚ï¸

1. The keys of **state (s)** are [arena ğŸŸï¸, cards ğŸƒ, elixir ğŸ’§]. The trained YOLO model gives inference on arena detection results and detected troops with keys [position ğŸ“, class ğŸ¯, belongings ğŸ’]. The ResNet model detects the cards table ğŸ´, showing only 4 cards at a time. cnocr ğŸ”¤ detects remaining elixir.

2. The keys of **action (a)** are [select ğŸ–ï¸, pos ğŸ“] representing selected cards and deployment positions in the arena. The arena is divided into 32x18 sub-blocks.

3. The key of **reward (r)** is the scalar reward ğŸ…. Positive reward if opposite tower HP is reduced ğŸ’¥, negative otherwise ğŸ˜¢.

4. Dataloading process: each timestep t â±ï¸ has (s, a, r) pairs. The sequential construction is necessary for transformer decoder model ğŸ”„.
   - Sequential Dataloader: Each sequence is length n (ex. 30), with the first 29 input pairs predicting the 30th pair ğŸ§©.

5. **Sparse Action Addressing âš¡**
   
 ![Frame Re-weighting](frame_weights.png)

The actual action frames get higher weights ğŸ‹ï¸â€â™‚ï¸, and neighboring frames get decaying weights â¬‡ï¸.

## Model Structure Discussion ğŸ—ï¸

## References ğŸ“š
